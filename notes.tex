\documentclass[a4paper]{book}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{listings}


\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{example}{Example}[section]
\newtheorem{claim}{Claim}[section]


% lstlisting macros
\lstset{
  language=Python, % choose the language of the code
  showspaces=false, % show spaces adding particular underscores
  showstringspaces=false, % underline spaces within strings
  showtabs=false, % show tabs within strings adding particular underscores
  tabsize=8, % sets default tabsize to 2 spaces
  captionpos=b, % sets the caption-position to bottom
  mathescape=true, % activates special behaviour of the dollar sign
  basicstyle=\footnotesize\tt,
  columns=fullfelxible,
  xleftmargin=2em,
  %commentstyle=\rmfamily\itshape,
  morekeywords={},
  escapeinside={(*@}{@*)},
  numbers=left
}

\title{Research Notebook}
\author{David R. MacIver}

\begin{document}

\maketitle

This is mostly a collection of research level material I am interested in.
It's something like a lab book,
something like a collection of papers I find interesting and want to make sure I remember and/or understand.

It is likely to be terribly organised.
It's primarily intended as a place for me to write stuff that is useful for me to have written down,
and only secondarily intended for public consumption.

It's also likely to be terribly formatted.
I'm not very good at \LaTeX.

\tableofcontents

\chapter{Property Based Testing Literature Review}

This is mostly an index of existing property-based testing related papers,
with some commentary.

The foundational paper is of course ``QuickCheck: a lightweight tool for random testing of Haskell programs''\cite{DBLP:conf/icfp/ClaessenH00}.
The core idea here is that you can express testing as if it were formal verification,
by using the type signature of a function to fill in random values.

I am not a fan of this idea,
and think that type driven property based testing is a historical accident that has largely served as an attractive nuisance that has prevented people from implementing better solutions.

``Smallcheck and lazy smallcheck: automatic exhaustive testing for small values''\cite{DBLP:conf/haskell/RuncimanNL08} is in some sense the other foundational paper in this area
but seems to have proven less popular (and I'm suspicious about its efficacy),
based on bounded exhaustive enumeration..

``EasyCheck---Test Data for Free''\cite{DBLP:conf/flops/ChristiansenF08} is an interesting extension where the generation is based on randomized tree enumeration,
where the trees are expressed with non-determinism operators (or just ADTs).

``Type Targeted Testing''\cite{DBLP:conf/esop/SeidelVJ15} is based on using refinement types to satisfy tricky preconditions.

``Targeted property-based testing``\cite{DBLP:conf/issta/LoscherS17} is about some specialisations for the case where the property depends on some numeric value.
Suppose you have some property \texttt{assert f(x) <= bound}.
This will tend to be falsified early or not at all---purely
random testing explores its boundary values very quickly (the probability of the maximum occurring at \(n\) tests is \(\frac{1}{n}\)).
But by doing a simple hill climbing algorithm based on mutating found values to increase their score,
this can very easily be found.

``Find more bugs with QuickCheck!''\cite{DBLP:conf/icse/HughesNSA16} is about the problem where certain bugs tend to dominate:
If you have one very likely bug,
it will tend to dominate your testing and distract you from finding other bugs.
In this paper (which is specifically about Erlang QuickCheck's model based testing) they propose using sequences of method calls found in a bug to mark examples to avoid when generating later.

``A PropEr integration of types and function specifications with property-based testing''\cite{DBLP:conf/erlang/PapadakisS11} is about PropEr,
the other Erlang property-based testing library.
I haven't actually read it yet.

In the other direction, ``The New Quickcheck for Isabelle''\cite{DBLP:conf/cpp/Bulwahn12} is about a QuickCheck for Isabelle,
which is a theorem prover

``FitSpec: refining property sets for functional testing''\cite{DBLP:conf/haskell/BraquehaisR16} is about using mutation of tested functions to see whether a set of properties is minimal and complete.

\chapter{Boltzmann Sampling}

Given some combinatorial class of objects \(\mathcal{C}\) with a notion of size,
we might want to sample from it in such a way that any two objects with the same size occur with the same probability.
Boltzmann Samplers are a class of samplers which have this property and are amenable to mostly automated constructions.

The idea is that we consider the generationg function \(f(z) = \sum\limits_n z^n C_n\),
where \(C_n =  |\{c \in C: |c| = n\}|\}\),
so the coefficient of \(z^n\) is the number of elements of \(\mathcal{C}\) with size \(n\).

The Boltzmann samplers for \(\mathcal{C}\) are then a family of samplers with parameter \(z\) which pick an element of size \(n\) with probability \(z^n \frac{C_n}{f(z)}\).

The nice thing about this is that they combine well.\ 
e.g.\ if we have two disjoint classes of \(\mathcal{C}\) and \(\mathcal{C}'\) then we can construct a Boltzmann sampler for \(\mathcal{C} \cup \mathcal{C}'\) as a weighted choice between the Boltzman samplers for each,
picking from \(\mathcal{C}'\) with probability \(\frac{f'(z)}{f(z) + f'(z)}\).

A neat thing I have observed\cite{falbs} that probably isn't novel but that I haven't seen elsewhere is that given a regular language expressed as a deterministic finite automaton
(e.g.\ by using derivatives!)
you can automatically calculate a Boltzmann sampler for it using symbolic linear algebra
(or non-symbolic linear algebra if you fix the parameter up front).

The similarity of this paper to~\cite{DBLP:journals/dam/FlajoletGT92} is probably not accidental given that they have an author in common---both
are about using generating functions of regular languages to simplify some natural probabilistic problem through semi-automatic constructions.

\section{Boltzmann Sampling over Possibly Infinite Unknown Automata}

Note for those who are following along at home:
This chapter is basically stream of consciousness.

Suppose we have some automaton with a finite alphabet \(\mathcal{A}\) and states \(S = \{s_0, \ldots, s_n, \ldots\}\) which may or may not be finite.
We have a single origin state \(s_0\),
a transition function \(\delta: \mathcal{A} \times \mathcal{S} \to \mathcal{S}\),
an an acceptance function \(a: S \to \{0, 1\}\).
This defines a language \(\mathcal{L}\) over \(\mathcal{A}\) in the usual way:
Start at \(s_0\), repeatedly move to the next state via the next character of the string,
if at the end we're in an accepting state the string is in the language.
Similarly each of the states defines the language \(\mathcal{L_i}\) of suffixes starting from that state,
with \(\mathcal{L} = \mathcal{L}_0\).

Let \(f_i\) be the Boltzmann generating function for \(\mathcal{L}_i\).
If we knew these exactly,
we could calculate a Boltzmann sampler for \(\mathcal{L}\) by defining the random walk that,
at state \(s_i\),
chooses between terminating (if the state is expected) with weight one,
or emitting \(c\) and transitioning to \(s_j = \delta(c, s_i)\),
with weight \(f_j(x)\).

Unfortunately, we will typically not know these generating functions!
For regular languages it is possible but expensive to calculate them,
but we have to find all of the states up front (of which there may be exponentially many),
and it's moderately expensive in the size.
For context-free languages the problem is uncomputable in general (I think),
and for languages of a completely unknown structure we're certainly stuck.

But with only a little information,
we can actually simulate the Boltzmann sampler fairly efficiently!

Suppose we have lower bounds \(l_i(x)\) and \(u_i(x)\) with \(0 \leq l_i(x) \leq f_i(x) \leq u_i(x)\) for all \(i\).
Suppose additionally we have \(u_i \leq u(x)\).

It is easy to find such bounds in many cases.

A natural choice of \(l_i\) is \(a\)---in an accepting state,
the Boltzmann function contains a \(1\) term for the zero-string.
In an non-accepting state we might not have any strings matching from here at all,
so without more information \(0\) is the best lower bound we can do.

One possible source of lower bound is that if we have some strings \(v_i\) in the language we can walk the graph and then use them to witness some suffixes of \(s_i\),
which will give us a lower bound by adding up the terms corresponding to them.
In general if we know that \(\mathcal{L}_i \supseteq \mathcal{L}'\) with generating function \(f_i'\) then \(l_i(x) = f_i'(x)\) is a suitable choice.

We have three potential sources of upper bounds:

\begin{itemize}
\item If we know that \(\mathcal{L} \subseteq \mathcal{L}'\) then every \(s_i\) is \(\mathcal{L}\)-equivalent to some state in a representation for \(\mathcal{L}'\),
so if we have some uniform upper bound \(A(x)\) on states in \(\mathcal{L}'\) then we also have it for states in \(\mathcal{L}\).
\item \(f_i\) is monotnoic increasing in \(x\), so if we know some upper bound for \(f_i(y)\) with \(y > x\) then we also know it for \(f_i(x)\).
\item If \(x < \frac{1}{|\mathcal{A}|}\) then \(f_i(x) \leq \sum {(x |\mathcal{A}|)}^n = \frac{1}{1 - x |\mathcal{A}|}\).
\end{itemize}

We can now use these bounds to simulate a Boltzmann sampler for any fixed value of \(x\) where \(u_i(x) < \infty\) for all \(i\)!

To do this we maintain \emph{bound tables} for each \(i\).
These are updatable tables such that at any point we guaranteee that \(L[i] \leq f_i(x) \leq U[i]\).
When we first evaluate \(L[i]\) or \(U[i]\),
we automatically initialise them to \(l_i(x), u_i(x)\) respectively.
We also maintain a visited table which tracks if we have ever evaluated \(\delta(c, i)\) (we assume that \(\delta\) is expensive but cached).
Let \(V[i]\) be the set of \(c \in mathcal{A}\) such that we have previously evaluated \(\delta(c, s_i)\).

We can now perform the random walk of a Boltzmann sampler without ever having to fully compute \(f_i(x)\) as follows:

When making a decision about what to do next (stop or transition) we sample as follows:

\begin{itemize}
\item Stopping here gets a weight of \(0\) or \(1\) as normal depending on whether \(s_i\) is an accepting state.
\item For each \(c \in \mathcal{A}\), we give it a weight according to some upper bound on \(f_{\delta(c, s_i)}\).
If we have previously evaluated \(\delta(c, s_i) = s_j\) then this is \(U[j]\).
Otherwise it is \(u(x)\).
If \(u(x) = \infty\) then we can just evaluate \(\delta(c, s_i)\) for each \(c\),
which is more expensive but allows us to never care about infinite weights.
We then draw an action with probability proportional to its weight.
\item If we drew ``stop'' then we stop as normal.
If we drew \(c\), then we determine \(s_j = \delta(c, s_i)\) if we haven't already.
We draw a number \(t\) uniformly at random in \([0, U[j]]\).
If \(t \leq L[j]\) then we emit \(c\) and transition to \(j\).
If \(t > L[j]\) then we call \texttt{improve(j, t)}.
After this call,
we will either have \(L[j] \geq t\),
or \(U[j] < t\).
If the former, transition with \(c\) as before.
If the latter, loop back to step 1 (that is, we stay at the current state but need to re-evaluate what to emit. Note that \(U[j]\) may have changed so we may need to update the sampling process).
\end{itemize}

The fact this this produces the correct result is fairly straightforward:
Conceptually what's happening is that we have numbers \(q_e \geq p_e\),
where \(p_e\) is the true probability of some event.
The probability of a single loop iteration choosing \(e\) is \(\frac{p_e}{q_e} \frac{q_e}{\sum q_{e'}} = \frac{p_e}{\sum q_{e'}}\).
So \(P(e| \text{ no repeat }) = p_e\).

\texttt{improve(i, t)} is a function that is designed to do just enough evaluation of the graph to put both \(L[i]\) and \(U[i]\) on the same side of \(t\).
It does this through a process of iterative refinement of lower bounds,
by using the recurrence relationship \(f_i(x) = a(i) + \sum\limits_{c \in \mathcal{A}} x f_{\delta(c, s_i)}\).

This is monotonic increasing in the values of \(f_{\delta(c, s_i)}\),
so in particular we can update the tables according to the following rules:

\begin{itemize}
\item \(L[i] \leftarrow \max(L[i], a(i) + x \sum\limits_{c \in V[i]} L[\delta(c, i)])\)
\item \(U[i] \leftarrow \min(U[i], a(i) + x \sum\limits_{c \in V[i]} U[\delta(c, i)]) + (|\mathcal{A} - |V[i]|)u(x)\)
\end{itemize}

\begin{proposition}
Once \(|V[i]| = |\mathcal{A}|\) and we have updated \(L[i]\) and \(U[i]\) according to the above rules,
\(U[i] - L[i] \leq x \max\limits_{s_j = \delta(c, s_i)} (U[j] - L[j])\).
In particular, \(U[i] - L[i] \leq x u(x)\).
\end{proposition}

I'm not sure of exactly the best way to get this process to converge at this point.
If you get the bound down to \(U[i] - L[i] < \epsilon\) then this halts the process with probability \(1 - \epsilon\),
but the question is how to get the bound down most efficiently?

Evaluating all states reachable in \(n = \lceil\frac{\log \epsilon - \log u(x)}{\log x}\rceil\) steps will achieve that.
This requires evaluating up to \(|\mathcal{A}|^n\) states,
although hopefully it wouldn't typically be that many in practice.

Ideas for improving it:

\begin{itemize}
\item We know how to handle self-loops perfectly, so we should use that.
\item Possibly this might be better phrased in terms of \(\frac{L[i]}{U[i]}\)?
That is the desired quantity to maximize, but it's less naturally expressed in terms of its children.
\item Use a greedy algorithm that just repeatedly tries to halve \(U[i] - L[i]\) by updating it,
evaluating as many unevaluated transitions as it needs, and recursing (avoiding loops) to the child with the largest contribution to its gap.
\end{itemize}

Idea:

Measure the ``overspill'', where \(o(i) = 1 - \frac{L[i]}{U[i]}\), or \(0\) if \(U[i] = 0\).
This is the probability of having to improve the bounds when sampling into state \(i\).

\(o(i) \leq \epsilon\) if and only if \(L[i] \geq (1 - \epsilon)U[i]\).

If \(L, U\) have been updated as above (along with the fix for self-reference),
suppose \(o(j) \leq \epsilon\) for every \(j\) immediately reachable from \(i\),
we can use this to bound \(o(i)\).
First assume \(i\) is not accepting:

\begin{align*}
o(i) &= 1 - \frac{x \sum L[\delta(c, i)]}{x \sum U[\delta(c, i)]} \\
&\leq 1 - \frac{x (1 - \epsilon) \sum U[\delta(c, i)]}{x \sum U[\delta(c, i)]} \\
&= \epsilon\\
\end{align*}

Now suppose \(i\) is accepting.
Let \(T = \sum \sum U[\delta(c, i)]\)

\begin{align*}
o(i) &= 1 - \frac{1 + x \sum L[\delta(c, i)]}{1 + x \sum U[\delta(c, i)]} \\
&\leq 1 - \frac{1 + x (1 - \epsilon) \sum U[\delta(c, i)]}{1 + x \sum U[\delta(c, i)]} \\
&= 1 - \frac{1 + x (1 - \epsilon) T}{1 + x T}\\
&= \frac{1 + x T - (1 + x (1 - \epsilon) T)}{1 + T}\\
&= \frac{x T - x T + \epsilon x T}{1 + T}\\
&= \epsilon \frac{x T}{1 + x T}\\
&\leq \epsilon \min(1, xT)\\
\end{align*}

So when \(xT\) is small,
we get a much tighter bound on the values for accepting nodes.

\begin{proposition}
Let \(f_i(x) = \sum\limits_{k \geq 0} C_k x^k\).
Then \(f_i(x) - \sum\limits_{k = 0}^n C_k x^k \leq {(|\mathcal{A}|x)}^{n + 1} u(x)\).
\end{proposition}

\begin{proof}
A string in the language is either of length \(\leq n\),
and thus appears in the first \(n\) terms,
or is exactly \(n + 1\) characters followed by the language of some state.
We thus have the remainder as \(\sum\limits_{|w| = n + 1} x^{n + 1} f_{\delta(w, s_i)} \leq {(|\mathcal{A}|x)}^{n + 1} u(x)\) as desired.
\end{proof}

We can also use this to look for probabilistic lower bounds if we're willing to approximate (which we probably should be).
If \(i\) is not accepting and for \(k \leq n\) the probability of a random string of length \(k\) being in \(\mathcal{L}_i\) is at most \(p\),
then we must have \(C_k \leq p |\mathcal{A}|^k\).
This then gives us an upper bound on the initial polynomial of \(p x |\mathcal{A}| \frac{{x |\mathcal{A}|}^n - 1}{x |\mathcal{A}| - 1}\).

This idea turned out to be horribly misleading!

The problem is that although low overspill guarantees that you will fairly accurately pick \(i\) when you draw \(i\),
getting to low overspill is not what you need to get out of the loop iteration!
It may well be that the true value is \(0\) but the initial estimate of the upper bound is really large.
We will \emph{never} get the overspill low unless we manage to prove the upper bound is \(0\),
but we don't need to---we just need to get the upper bound below the value we drew.

Here's an idea that might be worth pursuing:
The idea is that either the problem ``find the shortest path to an accepting state'' is mostly tractable or \emph{everything} we could do is hard.

Idea! Rather than having a generic \(u(x)\),
let's make this more concrete.

Given a language \(\mathcal{L}\) define the \emph{growth} of the langauge as \(g(\mathcal{L}) = \lim\sup \frac{\log |\{x \in \mathcal{L}: |x| = n\}|}{n}\).
Necessarily the growth is at most \(|\mathcal{A}|\).
i.e. \(g(\mathcal{L}) \leq h\) iff asymptotically \(|\{x \in \mathcal{L}: |x| = n\}| \leq h^n\).

The Boltzmann generating function is defined for any \(x < \frac{1}{g(\mathcal{L})}\) and undefined for any \(x > \frac{1}{g(\mathcal{L})}\).

Necessarily the languages of growth at most \(h\) are closed under differentiation:
If not, the set of strings starting with the differentiated prefix would eventually come to dominate the growth.

\section{Boltzmann Sampling of Unknown PRNG Languages}

Basic idea:

\begin{definition}
A PRNG language over some alphabet \(\mathcal{A}\) (typically \(\{0, \ldots, 255\}\)) is a prefix-free language \(\mathcal{L}\) with the property that every finite string \(s\) either \(st \in \mathcal{L}\) for some \(t\) or \(s = wt\) for some \(w \in \mathcal{L}\).
\end{definition}

Intuitively these are the strings read by some randomized function in its execution,
where we require the randomized function to have the property that it always has a non-zero probability of terminating.

The Boltzmann Sampler of parameter \(\frac{1}{|\mathcal{A}|}\) corresponds to the operation of reading a uniform infinite string and taking its prefix from the language.
However, this may not be well defined!

Constructing a Boltzmann Sampler for such a language can thus be thought of as ``taming'' unbounded random generators,
as they will be well defined for any \(x < \frac{1}{|\mathcal{A}|}\) while sharing many of the same properties---in
particular being uniformly distributed among anything with the same size.

I'm considering the following algorithm:

\begin{itemize}
\item Run enough of the normal random sampler through L* to get a good (lazy!) DFA for the language.
\item Simulate a Boltzmann Sampler for that language.
\end{itemize}

The core question is basically how accurate that Boltzmann Sampler is after some amount of L*.
We can correct the Boltzmann Sampler whenever it gets it wrong,
but we don't necessarily expect that to improve matters enough---it will improve the automaton to reject more strings,
but that won't necessarily cause it to accept more strings.

One thing that might help is the idea of a \emph{terminating sequence}.

\begin{definition}
A terminating sequence for a language \(\mathcal{L}\) is a (possibly infinite) sequence \(t \in \mathcal{A}^{\leq \omega}\) such that for any word \(w \in  \mathcal{A}^{\leq \omega}\), \(wt\) starts with a word of \(\mathcal{L}\).
\end{definition}

\begin{theorem}
Every PRNG language has a terminating sequence.
If the PRNG language is represented by a finite automaton of size \(n\) then it has a terminating sequence of length at most \(\frac{n(n - 1)}{2}\).
\end{theorem}

\begin{proof}
Let \(w_1, \ldots, w_n, \ldots\) be some enumeration of \(\mathcal{A}^{\leq \omega}\) and define \(t_n\) as follows:

\(t_0 = \epsilon\).
If \(w_n t_{n - 1}\) starts with some string in \(\mathcal{L}\),
\(t_n = t_{n - 1}\).
Otherwise,
because \(\mathcal{L}\) is a PRNG language,
\(w_n t_{n - 1} s \in \mathcal{L}\) for some \(s\).
Let \(t_n = t_{n - 1} s\).

Now define \(t\) as the ``limit'' of the \(t_n\):
If this process is eventually constant, \(t\) is just that constant value.
Otherwise \(t\) is an infinite sequence where \(t_i = {(t_n)}_i\) for sufficiently large \(n\).

If \(\mathcal{L}\) is regular we can use this process to construct a terminating sequence of size \(n^2\):
We only need to enumerate strings leading to each non-terminal state,
so there are only \(n - 1\) of them.
Order them as \(s_0, \ldots, s_n\) in order of increasing length of shortest path from the terminal state (so \(s_0\) is that state).
Necessarily the shortest path from \(s_i\) to the terminal state is at most \(i\)---if not it could not reach any state closer to it,
and so there is no extension of it in the language (contradicting that this is a PRNG language).
Thus we can construct our \(t_i\) above so that \(t_{i + 1} \leq t_i + i\),
and so the final length is \(t_n \leq \sum\limits_{i = 0}^{n - 1} i = \frac{n(n - 1)}{2}\)

\end{proof}

The bound \(\frac{n(n - 1)}{2}\) is tight if \(|\mathcal{A}| \geq n\):
Let \(a_i\) be some enumeration of the alphabet and \(s_i\) some enumeration of the states with \(s_n\) the terminal state.
Define transitions such that \(a_i\) causes \(s_i\) to transition to \(s_{i + 1}\) and for \(j \neq i\),
\(a_j\) causes \(s_i\) to transition to \(s_1\).

Note: In Hypothesis we assume that \(t = 0\ldots\) is a terminating sequence.

(I haven't actually checked that this works, but I'm pretty sure it does).

If we can efficiently find the initial prefix of some infinite sequence in the language (we typically can) we can use terminating sequences to do a sort of large fan-out version of L*,
where instead of classifying a string based on whether \(se \in \mathcal{L}\) we define the function \(f(s) = |w| - |s|\) where \(w\) is the unique prefix of \(st\) that is in \(\mathcal{L}\).
We then distinguish two states with respect to an experiment if \(f(se) \neq f(s'e)\).
This will tend to give us a very accurate picture of the automaton!

We can even do this if \(t\) isn't really a terminating sequence as long as it has a decent chance of terminating any string,
if we have an easy way of determining \(f_m(x) = \min(f(x), m)\) (which, again, we usually do).

For many PRNG languages this likely allows us to determine a perfect representation of the automaton without ever requiring a counter-example!

\chapter{Miscellaneous Literature Review}

This chapter is ``mostly'' literature review,
with the occasional sprinkling of my own work and ideas in it as they seem to fit.


\section{Inference of Finite Automata}\label{sec:langinference}

The paper I learned the most about this subject from was Rivest and Schapire's ``Inference of Finite Automata Using Homing Sequences''~\cite{DBLP:journals/iandc/RivestS93}.
This builds on Dana Angluin's classic L* search from ``Learning Regular Sets from Queries and Counterexamples''~\cite{DBLP:journals/iandc/Angluin87},
but extends it in several ways.

One very important feature of this paper which seems to be widely missed among people using L* search is that it significantly improves the algorithmic complexity of the number of queries that need to be performed.

Given some unknown language \(\mathcal{L}\) over an alphabet \(\mathcal{A}\),
L* search tries to find a deterministic finite automaton (DFA) representing that language given two oracles:
The first is a query oracle,
which allows you to test whether any string is in the language or not,
and the second is a counter-example oracle which allows you to test whether a given DFA is correct,
and if not presents a string for which it gives an incorrect answer.

It does this by constructing a deterministic finite automaton whose states are uniquely labelled by strings in some prefix-closed set \(S\),
and whose transitions are determined by some set of experiments \(E\).
Given a state \(s\) and a character \(c\),
the constructed automaton has a transition \(s \to t\) with \(c\) if \(sce \in \mathcal{L} \iff te \in \mathcal{L}\).
This works because of the following observations:

\begin{itemize}
\item Given \emph{any} deterministic finite automaton,
states can be uniquely labeled by a string that reaches that state from the origin.
By constructing these labels iteratively we can ensure that the set of labels used is prefix closed.
\item If two strings lead to the same state,
then any extension of them by the same string must also lead to the same state.
\item The same state cannot both be accepting and non-accepting.
\end{itemize}

Thus the experiments serve as a way of ``distinguishing'' two states:
If we ever observe that \(se \in \mathcal{L} \neq s'e \in \mathcal{L}\) then these can't lead to equivalent states.

This is the idea of the Myhill-Nerode theorem~\cite{nerode1958linear}\footnote{Confession:
I have not read the cited paper,
only the wikipedia page about it.
}.

L* search is essentially a way of learning the Myhill-Nerode automaton that results.
It proceeds in two parts:
The first tries to expand \(S\),
the second to expand \(E\).

Given a fixed set of experiments we first \emph{complete} the automaton:
For each \(s \in S\) and each \(c \in \mathcal{A}\) it tries to find a state in \(S\) that the current set of experiments thinks is equivalent to \(sc\).
If it finds one (and it cannot find more than one,
because we make sure to only enlarge \(S\) by adding inequivalent states to it),
we add a transition to that state.
If not,
we add \(sc\) to \(S\).

Once this is done,
we have an automaton that we hope matches \(\mathcal{L}\),
and we ask our counter-example oracle whether we're right.
If not,
we get a string which it gives the correct answer for.

This is where Angluin's original approach differs from Rivest and Schapire's.
Their observation builds on the surprisingly powerful idea that binary search has nothing to do with ordered sequences,
but is instead just about finding some point at which a function changes.\ 
i.e.\ if \(f(0) \neq f(n)\),
binary search finds some \(0 \leq i < n\) such that \(f(i) \neq f(i + 1)\).
If \(t\) is our counter-example then we can then use this to construct a new experiment to add to \(\mathcal{E}\) in \(O(\log(n))\) steps,
where \(n = |t|\).
We do this by taking \(f(i) = s_i t_{i+1} \ldots t_{n} \in \mathcal{L}\),
where \(t = t_1 \ldots t_n\) and \(s_i\) is the element of \(\mathcal{S}\) that labels the state that we are in after transitioning from the origin through
\(t_1, \ldots, t_i\).
We know that \(f(0) \neq f(n)\),
because \(f(0) = t \in \mathcal{L}\) and \(f(n) = s_n \in \mathcal{L}\) is the value that our automaton predicted for \(t \in \mathcal{L}\),
which was wrong.
This means that if \(i\) is our change point as above,
the transition we took from \(s_i \to s_{i + 1}\) was incorrect,
and \(t_{i + 2} \ldots t_n\) witnesses this fact,
so this is what we add to \(\mathcal{E}\).

In contrast,
Angluin's original algorithm tries to maintain \(\mathcal{E}\) as suffix-closed,
and as a result produces a much larger experiment set in a larger number of queries.

There is a lot of other information in this paper I need to go back and process at some point.
I should also write up its diversity based representation,
as I think it's a nice framing of this.

\section{DFA Minimization}

``Fast brief practical DFA minimization''~\cite{DBLP:journals/ipl/Valmari12} is a really nice paper about this,
although the experience of reading code golfed C++ to fit in a two column format wasn't a huge amount of fun.

It makes use of partition refinement,
applied to not just the states but the transitions,
beginning with a very coarse paritition that treats everything as equivalent and then progressively refining the partition whenever it finds an inconsistency.

One thing I thought was interesting about it is the core algorithm \emph{looks} like one you should have to iterate to a fixed point,
but actually it completes in a single run.

\section{Coupon Collecting and Sundry}\label{sec:coupons}

I've become interested in the non-uniform coupon collector problem recently,
which takes the following form:

Given \(X\) taking values in \(\{1, \ldots, n\}\) with \(P(X = i) = p_i > 0\),
if we have infinitely many independent copies \(X_i\) of \(X\),
and \(T = \min\limits_k |\{X_1, \ldots, X_k\}| = n\) (i.e. \(T\) is the first point we have seen every value at least once)
what is \(E(T)\)?

I proved some interesting lower bounds on this in terms of expectation that I haven't seen elsewhere but are probably not novel (I may add them in here later),
but more importantly ``Birthday Paradox, Coupon Collectors, Caching Algorithms and Self-Organizing Search''~\cite{DBLP:journals/dam/FlajoletGT92} is a really nice paper about this.

It takes the observation that this and many similar problems can be framed in terms of regular languages,
and that by using standard constructions of generating functions for regular languages,
you can more or less automate the calculation of their expected value (for values of ``automate'' that include calculating some potentially nasty integrals).

\section{Sandsifter}

Sandsifter~\cite{sandsifter} is a really neat trick that tries to find interesting hidden behaviour on x86 by just executing all the instructions and seeing what happens
(for values of ``seeing what happens'' that includes e.g.\ differential testing).
This seems infeasible:
Instructions may be up to 15 bytes long and there's no way to just try all \(256^{15} \approx 10^{36}\) such byte strings.

But it turns out that it's not!
The reason is that although they may be that long,
the instruction set is \emph{prefix-free}---no
instruction is a prefix of another.

This means that the following algorithm enumerates the entire instruction set:

\begin{enumerate}
\item Start from an all-zero buffer of 15 bytes.
\item Repeatedly execute the current buffer and see how many bytes are consumed.
\item Zero everything after the last byte read and increment the last byte read (handling overflow by setting it to zero and incrementing the previous one)
\item Stop when we have executed an instruction where every executed byte was 255.
\end{enumerate}

Because even though \emph{some} instructions are 15 bytes long \emph{most} aren't,
and in fact there turn out to be only about \(100,000,000\) instructions.

The only problem that remains is how to determine how many bytes were read given an x86 instruction.
This isn't something that the CPU tells you.
The answer is really neat:
Put the instruction right before a page boundary and watch to see if you trigger a fault when executing it!
If you do, then the instruction tried to read off the end of the page.
By sliding the position of the instruction around you can then find the smallest prefix that does not trigger a page fault.

\section{Parsing with Derivatives}

Parsing with derivatives\cite{DBLP:conf/icfp/MightDS11, DBLP:conf/pldi/0001HM16} is a really neat concept about how you can use laziness and meomization to extend the Brzozowski Derivative to context free languages,
essentially building up an infinite state automaton lazily as you traverse it.

I'm quite interested in applying this to the generation of objects in context free grammars,
but so far mostly just haven't---some
initial prototypes were reasonably promising but have never really manifested into anything very interesting.

One place this \emph{does} get used in Hypothesis is that the improved fixed point calculations in~\cite{DBLP:conf/pldi/0001HM16} are how Hypothesis calculates various properties of strategies which may recursively refer to each other.

\section{Fault Independence}

A nice set of papers starting from ``An Experimental Evaluation of the Assumption of Independence in Multiversion Programming''~\cite{DBLP:journals/tse/KnightL86}.

Key ideas:

\begin{itemize}
\item We want to see if multiple independent implementations of the same spec tend to fail in the same way.
\item Get a number of different people to implement the spec.
\item Then feed their implementations a million random inputs designed to look like a ``realistic workload'' and see if the inputs on which they fail are statistically independent.
\end{itemize}

Spoiler alert: They're not.

I haven't read the paper in detail yet,
but I \emph{have} read ``A reply to the criticisms of the Knight \& Leveson experiment''\cite{knight1990reply} which as well as being an intersting summary of the results is an absolutely glorious takedown.

As well as really liking the empiricism of this work,
one reason I'm interested in it is that a lot of the testing literature's focus on differential testing should be considered to have a fairly big hole down the middle as a result of this work---if everyone is making the same mistakes,
then differential testing won't pick this up,
and if mistakes are non-independent then the probability of this happening may be much larger than expected.

\section{Coverage Guided Fuzzing}

I need to do more research on this before I have anything intelligent to say about it,
but here are some things I think are neat ideas.

American Fuzzy Lop (AFL)~\cite{AFL} is of course kinda a big deal in this space.
There's quite a lot of literature on extending it,
though I'm not wholly convinced by it.

Nezha~\cite{DBLP:conf/sp/PetsiosTSKJ17} has the core idea that if you're doing differential testing between a bunch of different versions of something then maybe you should prioritise things that exhibit differences in behaviour!
It starts from some measurement \(\delta(x, s)\) taking an input and an SUT and returning some label for it,
then when doing differential testing between \(s_1, \ldots, s_n\) you can associate with an input \(x\) the profile \(\delta(x, s_1), \ldots, \delta(x, s_n)\).
Any particular value of this profile is not ``intrinsically'' interesting,
but the fact that two different inputs give a different profile indicates that the SUTs are exhibiting some difference in behaviour.
The important part of this is that they can do this without showing up any particularly novel behaviour in any one SUT!\@
I have not looked over their claims of its efficacy in any detail,
but it's an interesting idea.

Cause Reduction~\cite{DBLP:journals/stvr/GroceAZCR16} is the idea,
dear to my heart,
that you can apply test case reduction to many other problems,
not just bugs.
In particular you can apply it to coverage targets!
In this case they applied it mostly to coarse grained coverage targets (lines, branches, etc),
but I'm also interested in what happens with more fine grained ones.

\section{Constructive Mathematics}

``Five stages of accepting constructive mathematics''\cite{bauer2017five} is a hard paper to sumarize,
because I'm not sure it has a point per se.
Instead it's an interesting summary of some interesting aspects of and results from constructive mathematics.

Some of these results that caught my eye:

\begin{itemize}
\item It distinguishes \emph{proof by contradiction} from \emph{proof by negation}.
The former is a statement of the form ``If \(\lnot P\) then we reach a contradiction, therefore \(P\)''.
The latter is ``If \(P\) then we reach a contradiction, therefore \(\lnot P\)''.
Classically these are equivalent because \(\lnot \lnot P = P\),
but constructively they are distinct and proof by negation is still valid.
\item A lot of topology works nicely with locales in constructive mathematics.
e.g. Tychonoff's theorem still holds constructively for locales.
\item ``Any subset of a finite set is finite'' is equivalent to the law of the excluded middle.
\end{itemize}


\chapter{Miscellaneous Sums and Integrals}

The following came up in~\cite{DBLP:journals/dam/FlajoletGT92} as ``well known''.
It wasn't well known to me,
so I felt obliged to calculate it.
I'm not sure the process was enlightening enough to be worth it.

\begin{proposition}
\(\sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} \frac{1}{q} = H(m)\)
\end{proposition}

\begin{proof}
\begin{align*}
\sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} \frac{1}{q} &= \sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} \int\limits_0^1 x^{q - 1} dx\\
&= \int\limits_0^1 \sum\limits_{q = 1}^m {(-1)}^{q - 1} {m \choose q} x^{q - 1} dx\\
&= \int\limits_0^1 -x^{-1} \sum\limits_{q = 1}^m {m \choose q} {(-x)}^q dx\\
&= \int\limits_0^1 -x^{-1} \left( \sum\limits_{q = 0}^m {m \choose q} {(-x)}^q - 1 \right)dx \\
&= \int\limits_0^1 -x^{-1} \left( {(1 - x)}^m - 1 \right)dx \\
&= \int\limits_0^1 {(1 - x)}^{-1} (x^m - 1) dx \\
&= \int\limits_0^1 \sum\limits_{n = 0}^\infty x^n (x^m - 1) dx \\
&= \sum\limits_{n = 0}^\infty \int\limits_0^1 x^n (x^m - 1) \\
&= \sum\limits_{n = 0}^\infty \frac{1}{n + m} - \frac{1}{n} \\
&= \lim\limits_{k \to \infty}  H(m) - \sum\limits_{n = k}^{m + k} \frac{1}{n + m}\\
&= H(m)\\
\end{align*}

Where the conversion of the infinite sum to the limit works because for every \(n \geq m\) the positive term in the summand cancels the negative term from the summand for \(n - m\),
and the final limit is calculated because \(0 \leq \sum\limits_{n = k}^{m + k} \frac{1}{n + m} \leq \frac{m}{n}\).
\end{proof}

\chapter{Miscellaneous Research Questions}

\section{Lower Bounds on Collecting Coupons}\label{sec:couponbounds}

Given the setup of Section~\ref{sec:coupons} for the non-uniform coupon collector problem,
here is an investigation into lower bounds on the expected waiting time for a complete set of coupons.
There are many like it, but this one is mine.

\begin{lemma}\label{lemma:couponpartition}
Let \(U_i\) be the set of distinct values seen by time \(i\) and for \(A \subseteq \{1, \ldots, n\}\) let \(S^A\) be the number of times we have seen \(X_i \in A\) before we have seen every value in \(A\).
Note that in terms of probabilities this is the same as being \(S\) for the conditional distribution \(X | X \in A\),
but it's important that these are actually using the same \(X_i\).

Then \(E(S|U_i = A) = i + \frac{1}{1 - P(X \in A)} E(S^{A^c})\)
\end{lemma}

\begin{proof}
Let \(L_0 = i\) and \(L_{k + 1}\) be the first time after \(L_k\) that \(X_{L_{k + 1}} \not\in A\).

Then \(E(S | S^{A^c} = m) = E(L_m)\).
But \(L_{k + 1} - L_{k}\) is geometrically distributed with parameter \(P(X \not\in A) = 1 - P(X \in A)\),
so \(E(L_{k + 1} - E_(L_k)) = \frac{1}{1 - P(x \in A)}\),
and so \(E(L_m) = i + \frac{m}{1 - P(x \in A)}\).
By applying conditional expectation we thus have \(E(S) = i + \frac{1}{1 - P(X \in A)} E(S^{A^c})\) as desired.
\end{proof}

This partition result also gives u

\begin{theorem}
\(E(S) \geq n H(n)\), with equality if and only if \(X\) is a uniform distribution.
\end{theorem}

\begin{proof}
We will do this by induction.
It is trivially true when \(n = 1\) because \(S = 1\) in that case.

By applying the previous lemma,
\(E(S | X_1 = i) = 1 + \frac{1}{1 - p_i} E(S^{{\{i\}}^c})\).

\(S^{{\{i\}}^c}\) has the same distribution as \(S\) for the conditional distribution of \(X | X \neq i\),
so in particular it has the same expecation.
Thus by our inductive hypothesis,
\(E(S^{{\{i\}}^c}) \geq n H(n)\) with equality if and only if this conditional distribution is uniform.

Let \(b_i = E(S^{{\{i\}}^c})\).
Then by conditional expectation,
\(E(S) = 1 + \sigma \frac{p_i}{1 - p_i} b_i \geq (n - 1) H(n) \sigma \frac{p_i}{1 - p_i} \).

This sum can be seen to be strictly minimized when \(p_i = \frac{1}{n}\),
which gives us \(E(S) \geq 1 + (n - 1) H(n) \frac{n}{n - 1}\) with equality if and only if \(X\) is uniform.

A simple induction shows that this term is equal to \(n H(n)\) and the result is proved.
\end{proof}

\begin{proposition}\label{prop:coupontail}
If \(p = P(X \geq m)\) then \(E(T) \geq \frac{n - m + 1}{p}\).
\end{proposition}

\begin{proof}
This is just the lemma applied to the very crude lower bound that \(E(S^A) \geq |A|\).
\end{proof}

\begin{proposition}\label{prop:couponestimate}
For any strictly increasing function \(f\) and \(1 \leq m \leq n\)
\(E(S) \geq \frac{f(m) (n - m + 1)}{E(f(X))}\).
\end{proposition}

\begin{proof}
This follows straightforwardly from the standard result that \(P(X \geq m) \leq \frac{E(f(X))}{f(m)}\) and Corollary~\ref{prop:coupontail}.
\end{proof}

\begin{theorem}
\(E(S) \geq \frac{2 {(k + 1)}^{-1} {(n + 1)}^{k + 1} - e (k + 1) {(n + 1)}^{k - 1}}{E(X^k)}\)
\end{theorem}

\begin{proof}
Let \(f(x) = x^k\) in Proposition~\ref{prop:couponestimate}.

Now, define \(g(x) = (n - x + 1) x^k\).
We'd like to find \(m \in \{1, \ldots, m\}\) that maximises \(g(m)\) to get the largest lower bound on \(E(S)\).

\begin{align*}
g'(x) &= (n - x + 1) k x^{k - 1} - x^k\\
&= (n + 1) k x^{k - 1} - (k + 1) x^k\\
&= \left( (n + 1) k - (k + 1) x \right) x^{k - 1}\\
&= \left( (n + 1) \frac{k}{k + 1} - x \right)(k + 1) x^{k - 1}\\
\end{align*}

So \(g'(x) = 0\) when \(x = x_{\max} = \frac{k}{k + 1} (n + 1)\).

Unfortunately we can't just plug in \(x_{\max}\),
because it might not be integral.
But we can find an integer \(1 \leq i_{\max} \leq n\) with \(|x_{\max} - i| \leq 1\).
\(|g'(y)| \leq (k + 1){(1 + x_{\max})}^{k - 1}\) for \(y\) between \(x_{\max}\) and \(i_{\max}\),
so

\(g(i_{\max}) \geq (n - x_{\max} + 1)x_{\max}^{k} - (k + 1){(1 + x_{\max})}^{k - 1}\)

\begin{align*}
(n - x_{\max} + 1)x_{\max}^{k} &= \frac{n + 1}{k + 1} {(n + 1)}^k {(1 + k^{-1})}^k\\
&= {(k + 1)}^{-1} {(n + 1)}^{k + 1} {(1 + k^{-1})}^k\\
&\geq 2 {(k + 1)}^{-1} {(n + 1)}^{k + 1} \\
\end{align*}

Where the latter bound comes from the fact that \(x \to {(1 + x^{-1})}^x\) is monotonic increasing.

\begin{align*}
{(1 + x_{\max})}^{k - 1} &= {(n + 1)}^{k - 1} {(1 + k^{-1})}^k \\
&\leq e {(n + 1)}^{k - 1}\\ 
\end{align*}

Thus we have \(g(i_{\max}) \geq 2 {(k + 1)}^{-1} {(n + 1)}^{k + 1} - (k + 1) e {(n + 1)}^{k - 1}\).
Plugging this in to the results of the previous corollary,
this gives us the desired result.

\end{proof}

\begin{corollary}
If \(X\) comes from some family of distributions such that \(E(X^k)\) is bounded as \(n\) varies,
then \(E(S) \geq O(n^{k + 1})\).
\end{corollary}

This bound is not quite tight,
and is missing a logarithmic factor.
A more involved calculation could ``easily'' recover it---we
simply dropped the factor of \(H(n - m + 1)\) in our initial approximation of how long it would take to fill all \(k \geq m\).

These bounds are particularly interesting because \(E(S)\) is independent under permutations of \(\{1, \ldots, n\}\),
and thus we may reorder the \(p_i\) values however we want before calculating these bounds.

\begin{theorem}
Let \(\sigma\) be a permutation of \(\{1, \ldots, n\}\).
If \(f\) is strictly increasing on \([1, n]\) then \(E(f(\sigma(X)))\) is strictly minimized when \(\sigma(i) < \sigma(j)\) implies \(p_i \geq p_j\).
\end{theorem}

\begin{proof}
Suppose that \(\sigma\) does not have this property.
Pick \(i, j\) with \(p_i < p_j\) and \(\sigma(i) < \sigma(j)\).
Now create \(\sigma' = (\sigma(i) \sigma(j)) \cdot \sigma\),
the permutation that first performs \(\sigma\) and then swaps \(\sigma(i)\) with \(\sigma(j)\).

Now:

\begin{align*}
E(f(\sigma'(X))) - E(f(\sigma(X))) &= p_i \sigma(j) + p_j \sigma(i) -  p_j \sigma(j) - p_i \sigma(i) \\
&= (p_j - p_i) (\sigma(i) - \sigma(j)) \\
&< 0\\ 
\end{align*}

Where the latter inequality follows because \(p_j - p_i > 0\) and \(\sigma(i) - \sigma(j) < 0\),
by our assumptions.
\end{proof}

\begin{theorem}
Let \(X\) be such that \(p_{i + 1} \leq p_i\),
and let \(f\) be a strictly increasing function on \(\{1, \ldots, n\}\)
Then \(E(f(X))\) is strictly maximized when \(X\) is the uniform distribution.
\end{theorem}

\begin{proof}
A similar argument as the previous theorem: Suppose we have \(i\) with \(p_{i + 1} < p_i\).
Write \(p_i = a + b, p_{i + 1} = a - b\),
where \(a = \frac{p_i + p_{i + 1}}{2}\) and \(b = \frac{p_i - p_{i + 1}}{2}\).
Then if \(X'\) results from replacing both \(p_i\) and \(p_{i + 1}\) with \(a\),
this will result in

\begin{align*}
E(X') - E(X) &= a f(i) + a f(i + 1) - (a + b) f(i) - (a - b) f(i + 1) \\
&= b (f(i + 1) - f(i)) \\
&> 0\\
\end{align*}

So whenever we have two adjacent probabilities that are not equal,
we can strictly increase the expectation by making them equal,
thus this expectation is maximized by the uniform distribution.

\end{proof}

Thus with any strictly increasing function \(f\),
once we have reorderd the values of \(X\) in this way we can regard \(E(f(X))\) as a measure of ``how far from uniform'' \(X\) is.

\section{Boltzmann Sampling of Levenshtein Automata}

\newcommand{\levlang}[2]{\mathcal{L} (#1, #2)}

Let \(\mathcal{A}\) be an alphabet.
To avoid triviality assume \(|\mathcal{A}| \geq 2\).

The Levenshtein distance of two strings \(u, v\) over \(\mathcal{A}\), \(d(u, v)\), is the length of the shortest path from \(u\) to \(v\) where a step in the path is either replacing a single character, deleting as ingle character, or inserting a single character.

The set of strings \(\levlang{u, n} = \{ v: d(u, v) \leq n \}\) is finite,
and so certainly regular,
but it even admits a fairly small DFA (this is not a new observation at all).

This means we should be able to calculate a Boltzmann sampler for it\cite{falbs}!

Can we do so efficiently?

We'll use the Brzozowski derivative to explore the structure of these languages.

Let \(b_L\) be the generating function of a language \(L\).

First: A crosscheck we can use is that the coefficients in the generating function must always be polynomials,
and the coefficients must be in \([|u| - n, |u| + n]\),
as any single step can only change the size by at most one,
so there are no strings of length outside that range to contribute to the generating function.

First,
two useful special cases:

\begin{proposition}
Now note that \(\levlang{\epsilon}{n}\) is the set of all strings of length at most \(n\),
so has generating function \(b_{\levlang{\epsilon}{n}}(z) = \frac{1 - {(|A|x)}^{n + 1}}{1 - |A| z}\).
\end{proposition}

\begin{proposition}
\(\levlang{v}{0}\) matches only \(v\),
so has \(b_{\levlang{v}{0}} = z^{|v|}\) 
\end{proposition}

\begin{theorem}
Let \(v = cu\) where \(c \in \mathcal{A}\),
and let \(\delta\) be the Brzozowski derivative.

\begin{itemize}
\item \(\delta(\levlang{v}{n}, c) = \levlang{u}{n}\).
\item \(\delta(\levlang{v}{b}) = \levlang{cu}{n - 1} \cup \levlang{u}{n - 1} \cup \delta(\levlang{u}{n - 1}, b)\)
\end{itemize}

\end{theorem}

\begin{proof}
If \(b = c\) there is nothing to do that we couldn't do later,
so we can just consume it and match the rest of the string,
thus \(\delta(\levlang{cu}{n}, c) = \levlang{u}{n}\).

If \(b \neq c\) then we can either delete \(b\) from the string,
meaning that the suffix must now be in \(\levlang{cu}{n - 1}\),
replace it with \(c\),
meaning that the suffix must now be in \(\levlang{u}{n - 1}\),
or insert \(c\),
which means that the remainder of the string \emph{including the current character},
must be in \(\levlang{u}{n - 1}\),
i.e.\ the suffix must be in \(\delta(\levlang{u}{n - 1}, b)\).
So \(\delta(v, b)\) is the union of these three possibilities.
\end{proof}

\begin{proposition}
Decompose \(v\) as \(v_1 \ldots v_m\) where each \(v_i\) consists of repetitions of a single character \(c_i\),
and the character in \(v_i\) is distinct from that in \(v_{i + 1}\).
Let \(n = |v|\) and suppose \(n > 0\).

Then \(b_{\levlang{v}{1}} = m z^{n - 1} + (1 + (|\mathcal{A}| - 1)n) z^{n} + ((n + 1) |\mathcal{A}| - n) z^{n + 1}\)
\end{proposition}

\begin{proof}
This is a relatively straightforward counting argument.

You can obtain a string of length \(n - 1\) only by deleting a single character,
but deleting any character inside \(v_i\) has the same effect,
so there are only \(m\) distinct deletions.

You can obtain a string of length \(n\) only by leaving the string unchanged or replacing a single character,
and each index admits \(|\mathcal{A}| - 1\) such replacements.

The calculation for \(n + 1\) is slightly harder.
Such a string must be obtained by an insertion,
but we must be careful not to overcount.

First,
we count all the insertions that just increase the length of one of the \(v_i\) by \(1\).
This gives us exactly \(m\) new strings.

Now, there are \(n + 1\) points we can insert at.
If these are interior to a \(v_i\)---that is,
it is between two characters of the same value---
then there are \(\mathcal{A} - 1\) possible values to insert (anything other than \(c_i\)),
but if it is the boundary then there are  \(\mathcal{A} - 2\) values---anything
other than \(c_i\) or \(c_{i + 1}\).

There are exactly \(m - 1\) points that are not interior---in
between each of the \(v_i\).

Thus there are \((m - 1)(|\mathcal{A}| - 2) + (n - m + 2)(|\mathcal{A} - 1|) = (n + 1) |\mathcal{A}| - m - n\).
Counting the insertions that expanded a \(|v_i|\),
this gives us a total of \((n + 1) |\mathcal{A}| - n\) as desired.
\end{proof}

In general I doubt the existence of a nice formula for this Boltzmann Sampler,
but the nice thing is that we can use the derivative method to simulate it lazily.
We can also use the above formula to simplify our calculations whenever we get down to a \(\levlang{v}{1}\) term.

\chapter{Patches to L*}

Here are some ideas I have for how to improve L* search.
The subtext here is that I have a bunch of problems where I would like to use it but am currently unable to due to its cost.

\section{L* Search with Character Partitioning}\label{sec:lstarcharpartition}

As observed in ``Regular-expression derivatives re-examined''\cite{DBLP:journals/jfp/OwensRT09},
it can be extremely helpful to consider DFA transitions not as happening on a single character,
but instead a whole class of locally equivalent characters.

I think L* search (see Section~\ref{sec:langinference}) could benefit from the same observation!

Suppose we modified L* search as follows:
Whenever we create a state,
we associate a partition refinement data structure with it,
initially treating all characters in the alphabet as equivalent.

When completing the transition graph,
we then only consider one character (say the smallest, or a randomly chosen one) from each partition.

When giving a counter-example,
this gives us a transition \(s \to s'\) via a character \(c\),
and a new experiment \(e\) which distinguishes \(s'\) from the state that we \emph{should} have transitioned to.
Note that unlike in conventional L*,
that state might be in the table already!

We can now use this experiment to refine the partition that \(c\) belongs to,
by evaluating \(sc'e\) for every \(c'\) that we currently believe is equivalent to \(c\).
If they do not all give the same answer,
this splits the partition.
Note that in this case we do not need to add the experiment to our set of experiments!

This is still a correct algorithm for inference because it reduces to L* in the case where all partitions are fully refined,
and at each point where we invoke the teacher to give us a counterexample one of three things happens:

\begin{enumerate}
\item The teacher tells us our DFA is correct and we are done.
\item We find a new inequivalent state.
\item We strictly refine one of the partitions.
\end{enumerate}

The reason is that either the canonical element we chose for the partition also produces a wrong outcome,
in which case a new state is found as it usually is in L* search,
or \(c\) exhibited different behaviour with \(e\) than said canonical element,
so the partition is strictly refined.

Thus whenever the DFA is not correct,
we make progress,
and thus we eventually terminate in a correct answer.

How much of a complexity improvement is this?

Well, it depends.
It's not a strict improvement,
because we make more calls to the teacher,
but we potentially save a huge amount of time on query calls.

Suppose the minimal DFA has \(m\) states and the alphabet has size \(n\).
Let \(d_i\) be the number of equivalence classes of characters starting from state \(i\).
Equivalently,
\(d_i\) is the number of states reachable in a single transition from \(i\).

Then when we have \(k\) experiments,
classically L* search requires \(O(mnk)\) queries to complete its transition table (once we have found some suitable fraction of the available states).
This changes that to \(O(k \sum d_i)\).
In particular,
\(d_i \leq m\),
so we now only need to perform \(O(km \min(m, n)) \) queries to get completion (and possibly much less).

However,
we may have to perform an additional \(O(\sigma d_i)\) queries to the teacher,
because we now have refutation steps that only increase the number of refinements.

This might be fine.\
e.g.\ for my use case more queries to the teacher is in some sense what we \emph{want}---if
the idea here is that we are using the DFA to guide some fuzzing process then we'd ideally spend almost \emph{all} the time querying the teacher,
and the table rebuilding process is something we only do when things go wrong,
and performing \(256 m\) queries each time is hugely undesirable.

Note: This is very like ``Query Learning of Regular Languages over Large Ordered Alphabets''\cite{Kaizaburo2017LearnAut},
so certainly isn't a new category of idea,
but their approach is to try to learn intervals.
The set of discernible intervals may be much larger than the set of discernible sets,
so I don't think this is actually a good idea except in the sense that their approach scales to much larger alphabets when it works.
A similar thing is done in~\cite{DBLP:journals/corr/MensM15} with stronger assumptions on the teacher.

\section{L* with guessing}

I haven't quite pinned down what I mean by this,
but I have this intuition that the big problem with L* is that it is too afraid of looking bad in front of teacher.
That is,
it tries too hard to get the right answer,
incurring a large cost in number of queries to avoid a small cost in number of requests for counter-examples.
If you're in a situation like I am where counter-examples are either cheap or uninteresting,
this isn't a good trade-off.

So what if you just guessed an answer that was likely to be correct when doing so is cheap?

For example, imagine we have some plausible ordering \(e_1, \ldots, e_k\) of the experiments,
and we want to calculate which state \(s\) corresponds to.
Say discovery order + self-organising search or something.

Now suppose we want to determine the state corresponding to \(t\).
We run down our experiments,
repeatedly pruning the list of possible states.

What we are \emph{supposed} to do is to run every experiment,
but what if we just stopped when we got down to one state?

The answer is of course we would make more wrong transitions,
but maybe that's fine?

When our teacher points out that a transition was wrong,
we can just check if it was based on a guess,
and before we do anything else we run the rest of the experiments to see whether we already knew this should have been a novel state before adding a new experiment to the list.

\section{L* with local experiments}\label{sec:lstarlocal}

One of the big problems with L* is that it has to query every experiment against every state.
This gets expensive fast!

What if we instead maintained it as a decision tree?
Each state corresponds to a leaf in the tree,
and when we find an experiment that distinguishes some previously believed to be equivalent state from it,
we split the leaf into a branch.

The tree has the potential to become horribly unbalanced,
but the worst case scenario is that we perform exactly as many queries as we were going to before!

Although that's not technically true without a modification:
This could cause us to add more experiments than we previously would have.
When we discover a state needs to split,
perhaps we should first try the experiments that we've previously seen that were not on the path to that state before adding a new one?

\section{Recursive L*}

This is currently a \emph{very} half-baked idea (and if you're not me and reading this section it probably won't make much sense).

We have some prefix free language \(\mathcal{L}\), a classifier \(f: \mathcal{L} \to \mathbb{N}\),
and a random variable \(X \in \mathcal{L}\).
We would like to construct the conditional random variable \(X|f(X) = i\) more efficiently than using rejection sampling
(as we're specially interested in doing this in cases where \(P(f(X) = i)\) is small).

We are interested in particular in the language \(\{x: \exists i, P(X = i|X \text{ starts with } x) = 1\}\)---these
are the set of prefixes which are enough to fully define the behaviour of \(X\).
Note that unlike \(\mathcal{L}\) this is very much not prefix-free:
It is closed under taking extensions!

The idea I have been trying to figure out is whether we can do this with language inference,
by creating and then simulating a deterministic finite automaton based on the language.
This is not really viable,
both because language inference is intrinsically expensive (even with some of the above ideas) and because the structure of the language while technically regular (we have a size upper bound if we really need one) is potentially massively complicated.

The idea I'd like to explore in this section is whether we can make use of some of our knowledge about the structure and boundary information of the language to fix this.

In Hypothesis,
we keep track of example boundaries.
This has a couple of consequences:

\begin{itemize}
\item We can tell exactly which substring corresponded to a given draw call.
\item We can perform \emph{precise random replacements} of any draw call we choose---we
draw the same bytes up to that point and while the draw call of that index is active we return random bytes,
when it becomes inactive we read from the prefix.
\end{itemize}

Note that for any draw call,
the set of strings corresponding to the draw at that index in any test case which agrees with the prefix of this one is itself a prefix free language.

This means that at any index that is right after a completed draw call (counting bytes as draw calls here),
we have a prefix free language of bytes that can come next (and a generator for them!).

So we can consider this as a prefix-free regular language where the alphabet elements are not bytes,
but are themselves other prefix-free regular languages!

We have no hope of performing full blown L* here,
but maybe we don't need to!

We identify states not with strings as in regular L* but with a TestResult object plus an index into its draws and bytes calls.

A state may be in one of two err states:
Restricted and unrestricted.
Unrestricted basically means that we don't think the value emitted here matters.
We want as many states to be unrestricted as possible.
Unrestricted states are cheap.

A restricted state is one we've started building an automaton for,
recursively applying the process to the local language!

At the top level we work with the classifier that evaluates \(f(x0\ldots)\).
Either this will successfully measure whether starting with \(x\) always gives the same value of \(f\),
or we'll eventually find some string where it doesn't anyway.

When we recursively apply this to the language of a given state,
we instead apply it to the function that similarly zero-extends the string and then look at the state we end up in in the current state machine.

We can then generate a test case by recursively walking the state graph:
If the current state is unrestricted,
just generate it.
If it's restricted,
generate a random walk through its state graph that ends up with the desired classification,
then fill in by recursively generating an example from each of the transitions made.
Finally, because we only matched up to some prefix, extend the rest with random noise.

We can now check the results against our desired outcome.
If it agrees with it, great we're done.
If not we now begin a correction procedure.

In one sense this is much easier than L*,
because we know the exact point at which things wrong---where
we entered an accepting state.

But there are two possibilities here:
One is that we need to introduce a new state,
but another is just that our generator for the sub-language put us in the wrong place.
So first we have to check what state we actually ended up in.
If it's the right one,
then we add the experiment to our set and split off a new state.
Either way, we then recursively apply the correction procedure to the language for this state.

\section{Lazy L*}

I think this might be the idea that ties everything together.

The key notion here is that actually I don't care about the goal of L*---I
don't expect languages to be regular,
and I don't much care about their DFA per se.

This is false in that having a DFA of known structure would be extremely useful,
but given that I don't expect there to be one I think it makes sense to try to figure out how to do with something weaker.

The construction phase of L* can be thought of as separated out into two parts:

\begin{enumerate}
\item A process of partition refinement of the infinite set of strings\footnote{
As in Section~\ref{sec:lstarcharpartition} this is also a useful way of thinking about the behaviour at any given state.
}.
\item The construction of a DFA from partition refinement.
\end{enumerate}

For the partition refinement,
what we specifically have is that we have some infinite set \(X\) and a set of classifiers \(f_i: X \to \mathbb{N}\) and we want to get progressively better approximations to the partitions defined by \(p_x = \{y: \forall y, f_i(x) = f_i(y)\}\).

Suppose we had an partition refinement data structure that looked as follows:

\begin{lstlisting}
class PartitionRefinement(object):
  def __len__(self):
    """The number of currently known distinct partitions."""

  def __getitem__(self, i):
    """The canonical representative for partition i.
    This will never change - once a canonical representative
    is chosen it is fixed."""

  def partition_of(self, value):
    """Return the current partition of value. If it is not
    in any of the existing partitions, a new one will be
    created and value will be used as its canonical representative.
    """

  def split_partition(self, value, classifier):
    """Let i = self.partition_of(value). If classifier(value)
    != classifier(self[i]) then create a new partition with
    value as its canonical representative, and split the current
    partition so that any values in it with distinct outcomes
    for classifier(v) go in different partitions."""
\end{lstlisting}

L* search then implies a specific concrete implementation of this data structure that performs every operation fully strictly:
When you call \texttt{split\_partition} it appends it to a global list of experiments,
and when you look up a string it consults every one of those experiments to refine the partition.
However, other implementations are possible!
e.g. the local experiments idea in Section~\ref{sec:lstarlocal} is basically a suggestion of an alternate implemntation of this idea.
Additionally,
this API allows us to calculate the refinements \emph{lazily}.
We don't have to determine if \(s\) should actually go in a new partition until we're asked what partition \(s\) is in.

In L* we then build the DFA transitions eagerly as follows:

\begin{lstlisting}
def build_dfa(partition_table, characters):
  # This should have been set up at the beginning
  assert len(partition_table) > 0
  assert partition_table[0] == b''

  transitions = []

  # we might increase the number of partitions as we
  # go, so this can't be a normal for loop.
  i = 0  
  while i < len(partition_table):
    assert len(transitions) == i
    transitions.append([])
    s = partition_table[i]
    for c in characters:
      transitions[i][c] = partition_table.partition_of(s + c)
    i += 1
  return transitions
\end{lstlisting}

But there's no reason we have to build it eagerly!
One of the neat things about the implementation of regular expression matching in terms of the Brzozowski derivative is that you can build the DFA lazily as you match,
and we can do exactly the same thing here.

This is particularly useful if we want to somehow extract a subset of the automaton that is much smaller than the larger automaton!

For example,
suppose we had some set of strings \(U\) and we want to construct the automaton consisting of only states that are on the path for some \(x \in U\).
We can simply enumerate \(U\) and walk the automaton,
noting which states appear there,
and extract the subset of the automaton that contains only those states.

\chapter{The Plateau Problem in Random Testing}

This is something I'm thinking about but haven't quite figured out how to formalise.

``Why is random testing effective for partition tolerance bugs?''\cite{DBLP:journals/pacmpl/MajumdarN18} showed that under some circumstances,
random testing achieves a lot of different coverage targets very quickly.
However, their notion of ``very quickly'' involves a \(\frac{1}{p}\) term,
so when the lower bound on finding a particular coverage target is small,
this isn't actually quick at all.

This in particular applies when (in very stark contrast to the examples they're considering),
any two coverage targets are \emph{disjoint}---e.g.\ 
when the coverage target is a precise coverage profile for everything that was hit.

When this is the case,
necessarily for some coverage targets, \(p \leq \frac{1}{n}\),
where \(n\) is the number of distinct targets.

This now ties in with the non-uniform coupon collector problem:
Finding a covering set is equivalent to collecting all of the coupons!

Suppose we've got some random tester that emits values \(X\),
and we have some equivalence oracle \(c: X \to \mathbb{N}\) such that any two examples \(x, y\) with \(c(x) = c(y)\) are in some sense equivalent.
If the equivalence oracle is super-precise that might mean ``exhibit exactly the same behaviour and show exactly the same bugs'',
but it also might just mean something coarser like ``exhibit the same coverage profile'' or even just something like \(X|c(X) = i\) has some markedly different category of behaviour than \(X\).

The coupon collector problem, and in particular the bounds in Section~\ref{sec:couponbounds},
show that what we really want is for the distribution of \(c(X)\) to be uniform if \(c\) is precise (or, at least, if the number of truly inequivalent examples in each partition is roughly the same).
I'm pretty sure this is almost \emph{never} the case!

I have a sneaking suspicion that a lot of why things like American Fuzzy Lop and libfuzzer work is that what they are doing is effectively shifting to a new random tester when the old one appears to be exhausted,
and doing so in a way that the new random tester is complementary to the old one.
Effectively what they are doing by picking a new seed is sampling from some approximation to the conditional distribution \(X|c(X) = i\).
This is both potentially useful because it finds more of the inequivalent examples that could have been lurking in \(c(X) = i\) that were previously found with very low probability,
but also simply because the new distribution is different enough from the old one that it is likely to have a different set of examples that it finds within reasonable time.

\chapter{Active Reading}

This is a chapter of notes on books as I read them.

\section{How to Read a Book}

In ``How to Read a Book''\cite{ReadABook} they recommend a process of active reading,
in which rather than passively reading a book you actively engage with it through a process of outlinining and note-taking (or maybe they don't and I've made that up? I'm currently revisiting the book).

It would be interesting to contrast this with ``Writing to Learn''\cite{WritingToLearn} though sadly I think I've ditched my copy.
I believe I was not all that impressed with it despite liking the concept.

That may not be a fair summary, as I don't think I've actually read enough of ``How to Read a Book'' to give it one,
so this section is my active reading of ``How to Read a Book''.

They define four levels of reading:
Elementary, Inspectional, Analytical, and Synoptical.
Each of these builds on the preceding.

Most of the book is about Analytical reading,
which is a form of thorough reading for the purpose of gaining understanding.

Things I currently believe in part because of this book (or at least believe I believe etc):

\begin{enumerate}
\item If you are actively engaged in trying to understand a book, you should be taking notes on it.
\item You should try a shallow reading before you try a deep one.
\item In particular it may be worth e.g.\ reading the conclusion before the rest of the book!
Compare and contrast John Regehr's advice on reviewing papers in~\cite{ReviewingPapers}.
\end{enumerate}

\section{Behind Human Error}

``Behind Human Error'' is about how accidents happen,
and the fallacy of treating ``human error'' as an adequate explanation for those accidents.
The Safety Matters blog has \href{a good review of this book}{http://www.safetymattersblog.com/2013/07/behind-human-error-by-woods-dekker-cook.html}.

Its core thesis seems to be that humans are not the problem per se,
and that in most cases systems are only as robust as they are because of the constant local intervention of human experts working around the inadequacies and preconceptions of the design of the system.

It presents fifteen (!) premises in support of this:

\begin{enumerate}
\item ``Human error'' is an attribution after the fact.
\item Erroneous assessments and actions are heterogeneous.
\item Erroneous assessments and actions shoudl be taken as the starting point for an investigation, not an ending.
\item Erroneous actions and assessments are a symptom, not a cause.
\item There is a loose coupling between process and outcome.
\item Knowledge of outcome (hindsight) biases judgements about process.
\item Incidents evolve through the conjunction of several failures/factors.
\item Some of the contributing factors to incidents are always in the system.
\item The same factors govern the expression of expertise and of error.
\item Lawful factors govern the types of erroneous actions or assesments to be expected.
\item Erroneous actions and assessments are context-conditioned.
\item Enhancing error tolerance, error detection, and error recovery together produce safety.
\item Systems fail.
\item Failures involve multiple groups, computers, and people, even at the sharp end.
\item The design of artifacts affects the potential for erroneous actions and paths towards disaster.
\end{enumerate}

I do not currently understand all of these but those I do understand seem sensible.

Some key concepts I've taken in so far:

\begin{itemize}
\item Error cannot be studied without studying normal operation in the absence of error.
\end{itemize}

From the Safety Matters review we have the following summarized definition that helped me a lot:

\begin{quotation}
An organization's sharp end is where practitioners apply their expertise in an effort to achieve the organization's goals.  The blunt end is where support functions, from administration to engineering, work.  The blunt end designs the system, the sharp end operates it.
\end{quotation}

Questions I'd like to think about when reading this book:

\begin{itemize}
\item How does this relate to software bugs?
\item How does this relate to softare ops when you are monitoring a single ``living'' system rather than shipping software?
\item How does this all relate to the concept of normalisation of deviance\cite{NormalisationOfDeviance}?
It sure sounds like the core thesis of the book could be uncharitably read as ``Normalisation of Deviance is Good, Actually''.
The term does not seem to appear anywher 
\end{itemize}

It seems to me that software development is in some sense inherently operating at the blunt end,
while ops is the sharp end,
but that doesn't feel quite right either.

In many ways I think the problem is that the notion of sharp vs blunt end is not quite right,
and instead there is a sort of fractal organisational structure where the blunt end is itself composed of sharp and blunt ends,
but even that's not quite right.

Possible I'm just being resistent to labelling as usual.

Notable quotes:

\begin{itemize}

\item ``The sources of successful operations of systems under one set of conditions can be what we label errors after failure occurs.''
\item ``Ironically, understanding the sources of failure begins with understanding how practitioners create success and safety first.''
\item ``Under resource pressure, however, any safety benefits of change can get quickly sucked into increased productivity,
which pushes the system back to the edge of the performance envelope.
Most enefits of change, in other words, come in the form of increased productivity and efficeincy and not in the form of a more resilient, robust, and therefore safer, system.'' (p. 247). This is the ``Law of Stretched Systems''.
\item ``Improper computerization can simply exarcerbate or create new forms of complexity to plague operations.'' (p. 248)
\item ``A basic pattern in complex systems is a drift toward failure as planned defenses erode in the face of production pressures, and as ar esult of changes that are not well-assessed for their impact on the cognitive work that goes on at the sharp end.'' (p. 249)
\item ``All cognitive systems are finite (people, machines, or combinations).
All finite cognitive systems in uncertain changing situations are fallible.
Therefore, machine cognititve systems (and joint systems across people and machines) are fallible.'' p. 144, cites~\cite{0849339332} p. 176.
\item ``computerization and automation integrate or couple more closely together different parts of the system'' p. 144
\item ``[an agent is a] computer program whose user interface is so obscure that the user must think of it as a quirky, but powerful, person'' p. 147. Page also contains a great discussion of questions users are forced to ask about interfaces.
\item ``practitioners tailored their strategies and behavior to avoid problems and to defend against device idiosyncrasis'' (p. 151).
\end{itemize}

Notes on reactions:

\begin{itemize}
\item Fundamentally I'm having a \emph{very} hard time believing the claim that people care about and actively work towards safety.
\end{itemize}

\chapter{Reading List}

Some things I should get around to reading but currently haven't,
in no particular order of priority.

This is a rather crude way of bookmarking papers and books.
It's mostly to get them into the bibtex and somewhere on permanent record rather than leaving them in Browser tabs waiting to be lost.


\begin{itemize}
\item More of the work of \href{Nancy Leveson}{http://sunnyday.mit.edu/papers.html}
\item The work of \href{http://mcs.open.ac.uk/hcs2/}{Helen Sharp}.
\item Dilemmas in a general theory of planning\cite{rittel1973dilemmas}.
\item Computers and language learning: An overview\cite{warschauer1998computers}
\item Nonparametric estimation of the number of classes in a population\cite{chao1984nonparametric} (probably going to be more interesting to read things that cite it, as this is quite old, but might be worth understanding the paper first).
\item Royens proof of the Gaussian correlation inequality\cite{latala2017royen}
\item Birds of the Internet: Towards a field guide to the organization and governance of participation\cite{fish2011birds}.
\item Mechanizing exploratory game design\cite{smith2012mechanizing}.
\item Can a machine design?\cite{cross2001can}
\item A bunch of work by \href{http://www.robertfeldt.net/}{Robert Feldt} and Simon Poulding on test case generation
(oh look! Something actually relevant to my PhD!).
In particular~\cite{DBLP:conf/icst/PouldingF15, DBLP:conf/issre/FeldtP13, DBLP:conf/icse/FeldtP15}.
\item Tools for Discovery, Refinement and Generalization of Functional Properties by Enumerative Testing\cite{matela2017tools}.
\item Probabilistic DFA Inference using Kullback-Leibler Divergence and Minimality\cite{DBLP:conf/icml/ThollardDH00}
\item Inference of regular languages using state merging algorithms with search\cite{DBLP:journals/pr/BugalhoO05}.
\item Hitting Families of Schedules for Asynchronous Programs\cite{DBLP:conf/cav/ChistikovMN16}
\end{itemize}

\chapter{Topological Barriers to Decision Making}

This is a dump of an old draft paper I wrote that never really went anywhere.

Its core message is that the idea of Von Neumann-Morgenstern rationality is aphysical,
and that even a logically omniscient agent cannot exhibit it if it is required to make only finitely many observations of the world.

I still think it has some interesting ideas but nobody else seems to agree with me,
which makes my research notes which nobody else reads a perfect place to put it\ldots

This paper shows that the Von Neumann-Morgenstern axiomatization of rationality
cannot correctly model logically omniscient agents who lack a priori
knowledge of the consequences of their choices but must instead determine
those consequences through experimentation.

I show that under some very mild restrictions on the behaviour of the agent
and the experiments available to it, topological considerations guarantee
that to an external observer that knows the the true probabilities of outcomes,
such an agent will always exhibit behaviour that appears irrational.

In particular I will show that for any agent limited to performing only a finite
(but possibly unbounded) number of experiments chosen from some countable set:

\begin{itemize}
\item If the experiments satisfy a very weak naturalness requirement, the agent
cannot exhibit \textit{any} utility function.
\item If the experiments satisfy a slightly stronger naturalness requirement,
the agent will exhibit a rather
pleasing continuity property that is very different from the behaviour exhibited
by a VNM rational agent.
\item ``Most'' utility functions are impossible for the agent to exhibit.
\end{itemize}

All of these results hold regardless for any agent, regardless of whether
its true underlying preferences are rational or not.

\section{Agent Design}

In this article I follow the set up of the Von Neumann-Morgenstern (henceforth,
VNM) theorem:

There is a finite set of outcomes $\mathcal{O} = \{O_1, \ldots, O_k\}$, and
an agent is given the opportunity to express preferences over lotteries (probability
distributions) over these outcomes.

That is, we have the set $\mathcal{L} = \{p \in \mathbb{R}^k, p_i \geq 0, \sum p_i = 1\}$,
and given \(U, V \in \mathcal{L}\), the agent must specify which of the two
it ``prefers''.

The VNM theorem says that under some reasonable assumptions,
any consistent set of preferences must take the form of computing and comparing
the expected value of some utility function. That is, the agent has a function
$\nu : \mathcal{O} \to \mathbb{R}$, and picks the lottery corresponding to whichever
of $E(\nu(U))$, $E(\nu(V))$ is higher, or expresses indifference between the two.

Commonly this is viewed as expressing a preorder $\preceq$ over lotteries. For
the purpose of this article it will be convenient to instead view it as a function
which returns a choice. That is, we have some choice set $\mathcal{D} = \{L, R, I\}$,
where $L, R, I$ are distinct atoms that are interpreted to mean ``Pick the left hand lottery'',
``Pick the right hand lottery'' and ``I don't care, pick either one''. An agent
then defines a decision function $d: \mathcal{L}^2 \to \mathcal{D}$.

The VNM theorem is primarily concerned with logically omniscient agents with
perfect information: The agents are entirely aware of the probabilities, and are
fully capable of thinking through all implications of their preferences up front
and enforcing any consistency requirements, no matter how complicated.

In the model I will consider here, the agent may still be logically omniscient
but no longer has perfect information. In particular the agent making the choice
does not have direct access to the lotteries, and must instead make a decision based on
observing experimental outcomes that are derived from them:

\begin{definition}
An experimental model is a sequence of random variables 
$E^{p, q} = (E^{p, q}_1, E^{p, q}_2, \ldots, E^{p, q}_n, \ldots)$, where
$(p, q) \in \mathcal{L}^2$ and the $E^{p, q}_i$ take values in some finite
sets $\mathcal{E}_i$. Let $\mathcal{E} = \prod \mathcal{E}_i$.

The $E^{p, q}_i$ may not necessarily be independent.

An agent is a partial function
$d: \mathcal{E} \to \mathcal{D}$ such that:

\begin{itemize}
\item $P(d(E^{p, q}) \text{is defined}) = 1$ for any $p, q$.
\item For any $x \in \mathcal{E}$ where $d(x)$ is defined
there is some $n$ such that if $d(y)$ is defined and $y_i = x_i$
for $i \leq n$ then $d(x) = d(y)$.
\end{itemize}

An agent is called total if it is defined for any element of $\mathcal{E}$.
\end{definition}

You can imagine an agent as a machine which repeatedly picks from the
following moves:

\begin{itemize}
\item Run a new experiment and observe its output.
\item Halt and return an answer in $\mathcal{D}$.
\end{itemize}

With the requirement that the agent is defined with probability $1$ being
equivalent to the requirement that the agent always halts.

This condition is equivalent to requiring that the agent be a continuous
function when the $\mathcal{E}_i$
and $\mathcal{D}$
are given the discrete topologies and $\mathcal{E}$
is given the usual product topology.

The definition of an agent as a function means that we are only considering
deterministic agents, but this is fairly harmless: If we imagine an agent
making the above moves with an additional move available to it of making
a non-deterministic choice, we can simply parametrize it by some fixed
external source of non-determinism and consider the resulting deterministic
agent (or, indeed, include the non-determinism in the experiments available
to it).

Given such an agent, what does it mean for the agent to be rational?

\begin{definition}
Given a utility function $\nu$, the decision function $k_\nu(p, q)$ returns
$L$ if $E(\nu(p)) > E(\nu(q))$, $R$ if $E(\nu(p)) < E(\nu(q))$, or otherwise
$I$. i.e. it picks whichever of the lotteries gives the highest expected
value.

An agent $d$ models $\nu$ if 
for all $p, q$, $P(d(E^{p, q}) = k_\nu(p, q)) = 1$. i.e. based on its
experimental observations it determines enough information about the underlying
lotteries to calculate the correct answer for that utility function

The agent weakly models $\nu$ if equality need only hold when $k_\nu(p, q) \neq I$.
i.e. the agent is allowed to choose arbitrarily when the correct answer is
indifference. Such an agent will still make decisions which maximize $\nu$ because
it will only make an different choice from a rational agent when the the
choice it makes doesn't matter. In particular the expected utility of its choice
is identical to that of a rational agent.

If an agent correctly models some utility function then it is called rational.
If it weakly models some utility function it is called weakly rational.
\end{definition}

Some agents aren't very interesting:

\begin{definition}
An agent is called trivial if it always constantly returns the same answer.
\end{definition}

Note that a rational agent modelling a constant utility function is trivial,
and this is the only trivial VNM rational agent.

Unfortunately it will turn out that agents modelling a utility function
are very much the exception and not the norm.

\section{Physically Plausible Agents}

We'll start by analysing some conditions on experiments that are required
to make them correspond to physically plausible knowledge about the world.
In the next section we'll then see what happens if we relax some of these
assumptions.

The character of experiments in the physical world is that they only ever
give you a finite amount of precision: That is, they never allow you to
\textit{precisely} determine a value $x$,
only that $||x - y|| < \epsilon$
for some $y, \epsilon$,
or, equivalently, that $x$
is a member of some open set.

So the appropriate notion for our experiments being physically plausible
is that if an agent ever witnesses a result it can only place the true
value to within some open set:

\begin{definition}
Call a set of experiments ``natural'' if for any finite sequence
$e_1, \ldots, e_n$,
the set $\{(p, q): P(\forall i \leq n, E^{p, q}_i = e_i) > 0\}$
is an open subset of $\mathcal{L}^2$.
\end{definition}

As we'll see shortly, weak rationality requires agents to perform
unnatural experiments.

But first note that no non-trivial natural set of experiments is
deterministic.

\begin{theorem}
If $E^{p, q}$
are a natural set of experiments such that for any $p, q$
there is some $t^{p, q}$
with $P(E^{p, q} = t^{p, q}) = 1$.
Then the value $t^{p, q}$
does not depend on $p, q$.
\end{theorem}

\begin{proof}

Fix $n$.

For $e \in \mathcal{E}_n$, let $A_e = \{(p, q): t^{p, q}_i = e\}$.

By naturalness this is an open set (it is a union of open sets over
all possible values for the first $n - 1$ elements of the sequence).

Further, if $e \neq d$ then $A_e \cap A_d = \emptyset$
by the definition of $A_e$.

Also, $\bigcup A_e = \mathcal{L}^2$
(because $(p, q) \in A_{t^{p, q}_n}$).

But $\mathcal{L}^2$
is connected, so it cannot be partitioned into disjoint non-empty
open sets. Thus all but one of these sets must be empty and we have
that $t^{p, q}_n$
takes the same value for all $p, q$.

$n$
was arbitrary, so we must have that $t^{p, q}$
takes the same value for all $p, q$.
\end{proof}

We'll see that this non-determinism is insurmountable, and agents cannot
hope to compensate for it adequately to be even weakly rational.

First we'll need a tool for analysing the structure of agents.

\begin{definition}
For any agent $d$, a trace of $d$ is a finite sequence $t = (t_1, \ldots, t_n)$
such that:

\begin{itemize}
\item $d$ is defined for at least one infinite extension of $t$.
\item $d$ is constant on the set any $x$ such that $d(x)$ is defined
and $d_i = t_i$ for all $i \leq n$
\item No proper prefix of $t$ has the above properties.
\end{itemize}

The value of a trace $t$ (for $d$) is the unique output that $d$ assigns to any
sequence starting with $t$.
\end{definition}

You can think of traces as being the point at which an agent that is repeatedly
drawing experiments stops and terminates.

\begin{proposition}
If $d(x)$
is defined then $x$
starts with a unique trace of $d$.
\end{proposition}

\begin{proof}
To see that any such trace is unique, note that by minimality no
trace is a proper prefix of another, so $x$
can only start with at most one trace.

By assumption there is some $n$
such that for any $y$
with $d(y)$
defined, if $x_i = y_i$
for $i \leq n$
then $d(x) = d(y)$.

Let $n$
be minimal with that property. Then $x_1, \ldots, x_n$
is a trace by definition.
\end{proof}

\begin{theorem}
If $d$ is a non-trivial agent performing natural experiments
then there is an open set $U \subseteq \mathcal{L}^2$
such that for $(p, q) \in U$,
there are two different values $c \neq c'$
such that $P(d(E^{p, q}) = c) > 0$
and $P(d(E^{p, q}) = c') > 0$.

i.e. the agent is non-deterministic on some open set.

In particular if an agent performing natural experiments is
weakly rational then it is trivial.
\end{theorem}

\begin{proof}
Let $d$ be any agent using the experimental model. 

As before the sets
$A_c = \{(p, q): P(d(E^{p, q}) = c) > 0\}$
are a cover of $\mathcal{L}^2$

But these sets are open:

$A_c = \bigcup\limits_t \{(p, q): P(d(E^{p, q}) \text{ starts with $t$}) > 0\}$

where $t$
ranges over the traces of $d$
that have value $c$.

Each of the sets in this union are open by the naturalness requirement,
so $A_c$ is a union of open sets and thus open.

But $\mathcal{L}^2$
is a connected topological space, which means it cannot
be partitioned into disjoint open sets. So either two of these sets are empty
(which means one of these sets must be the whole set and $d$
constantly takes that value) or two of them intersect. i.e. we can find some
$c \neq c'$
with $A_c \cap A_{c'} \neq \emptyset$.

As required, for any value in this set it returns either of $c$
or $c'$
with non-zero probability, and it is a finite intersection of open sets
and so open.

The ``in particular'' follows because the set of points to which a non-constant
utility function  is indifferent has empty interior (you can always move
an infinitesimal amount in the direction of preference), so the
above set cannot be contained within it. Thus there is some point
to which the utility function is not indifferent to which the agent assigns
more than one possible outcome.
\end{proof}

In case the ``natural" requirement seems too strong, note that the following
two examples satisfy it.

\begin{example}
Let $t \in (0, \frac{1}{2})$.

Let $U_i$
be some enumeration of a basis for the topology of $\mathcal{L}^2$
(e.g. balls with rational radius and rational coordinates)
and let $E^{p, q}_{i, j}$
be independent random variables such that if $(p, q) \in U_i$
they return $0$ or $1$
with equal probability
and if $(p, q) \not\in U_i$
they constantly return $0$.
The $j$ serve to force each $U_i$ to appear a countably infinite
number of times and have no other affect on the distribution.

This is natural because for any finite sequence $t_1, \ldots, t_n$
the set of values for which it is possible is the intersection of
the $U_i$
for which $t_{i, j} = 1$
for some $j$
(with an empty intersection being the whole set).
\end{example}

This example is particularly interesting for the following reason:

\begin{proposition}
There is a set $A \subseteq \{0, 1\}^{\mathbb{N}}$
and a continuous function $f: A \to \mathcal{L}^2$
such that for all $p, q$
$P(E^{p, q} \in A) = 1$
and $P(f(E^{p, q}) = (p, q)) = 1$
\end{proposition}

\begin{proof}
Let $A_{p, q}$
be the subset of $\{0, 1\}^{\mathbb{N}}$
such that $e_{i, j} = 1$
for some $j$
if $(p, q) \in U_i$.

Then $P(E^{p, q} \in A_{p, q}) = 1$,
because the probability of infinitely many Bernoulli random variables all
turning up $0$
is $0$,
so the complement of $A_{p, q}$
is a countable union of probability $0$
sets and thus itself probability $0$.

So if $A = \bigcup A_{p, q}$
then certainly $P(E^{p, q} \in A) = 1$.

Define $f(e)$
to be the unique element of $\bigcap \{U_i: \exists j, e_{i, j} = 1\}$

This intersection is non-empty because for \(x \in A_{p, q}\)
we have $(p, q)$
in the intersection, and unique because for $(p, q) \neq (p', q')$
there is some $U_i$
containing one but not the other. Similarly the fact that $(p, q)$
is in the intersection shows that $f(e) = (p, q)$
for $e \in A_{p, q}$
and we've already shown that $P(E^{p, q} \in A_{p, q}) = 1$.

This function  is continuous: Suppose $U_i \ni (p, q)$
and $f(x) = (p, q)$.

Then $x_{i, j} = 1$
for some $j$.

Thus the set $V = \{e: e_{i, j} = 1\}$
is an open neighbourhood of $x$
with $f(V) \subseteq U_i$.
\end{proof}

So in this example we have a continuous function that with probability $1$
recreates the original values of the lottery, but this is still not
sufficient to make a decision accurately.

Note that $V$ is open even if we give $\{0, 1\}$
the Sierpinski topology (where the only non-trivial open set
is $\{1\}$),
as are the sets that we used to prove that this was natural,
so this example works even if we weaken the condition that
the topology on the experimental outcomes is discrete.

\begin{example}
Let $E_k$ be an independent series of random variables which each sample from the two
lotteries, returning a value in $\mathcal{E}_k = \mathcal{O}^2$.

That is, our agent can simulate
the outcomes as many times as it wants, it just can't know the exact probabilities.

This is natural because the probabilities of initial sequences are all
polynomial in $p, q$.
The law of large numbers guarantees we can use this to approximate $p, q$
with high precision and high probability.
\end{example}

i.e. the model subsumes both the case where we can measure  the lotteries to arbitrary
precision with a small chance of error and the case where we can sample from the
lotteries an arbitrary number of times.

Both of these models allow you to approximate the true lotteries to any
finite level of precision with any probability arbitrarily close to (but not exactly
equal to) $1$.

Both of these are substantially more powerful measurements than we can hope to have in
most realistic scenarios, so this model should be more than sufficient to define some
bounds on the behaviour of agents.

The second actually satisfies a much stronger criterion than our naturalness
requirement:

\begin{definition}
If for all finite sequences $e_1, \ldots, e_n \in \mathcal{E}_1 \times \ldots \times \mathcal{E}_n$
the functions $(p, q) \to P(\forall i \leq n, E_i = e_i)$ are continuous then 
the experimental model is called a sampling model.
\end{definition}

Certainly any sampling model is natural (because the sets defining the naturalness
requirement are then the preimage of open sets under a continuous function).

The first gives an example of a natural model which is not sampling: The probabilities
change discontinuously as you cross the boundary of one of the defining balls.

Sampling models obey quite a strong continuity requirement that no
VNM rational agent satisfies.

First we'll need a technical lemma.

\begin{lemma}
Let $t_i$ be the traces of some agent. Then 
$\sum\limits_i P(E^{p, q} \text{starts with $t_i$}) = 1$.
\end{lemma}

\begin{proof}
Because of the observation that no trace appears as the prefix of another, these are disjoint events.

That means this sum is $P(E^{p, q} \text{starts with any trace})$.
But if an agent terminates on a sequence of experiments then the
sequence must start with a trace - just pick the shortest prefix of that value
after which the agent is constant.

Therefore this is
$P(\text{the agent terminates when experiments are drawn from $E^{p, q}$ })$,
which is defined to be equal to $1$.
\end{proof}

\begin{theorem}
For an agent $d$
using a sampling model, for each $c \in \mathcal{D}$
the function $f_c(p, q) = P(d(E^{p, q}) = c)$
is continuous function $f_c: \mathcal{L}^2 \to [0, 1]$.
\end{theorem}

\begin{proof}
Let $t_1, \ldots, t_k, \ldots$
be a (possibly finite) enumeration of all the agent's traces.

Now, let $u_i = 1$ if given the trace $t_i$ the agent outputs $c$, else $u_i = 0$.

We have $f_c(p, q) = \sum u_iP(E^{p, q} \text{starts with $t_i$})$, because any value that starts with trace $t_i$ must have the same output.

Now we just need to show this sum is continuous.

Fix $p, q$.
Pick some $N$ such that $\sum\limits_{i \geq N} P(E^{p, q} \text{ starts with } t_i) < \frac{1}{3}\epsilon$
(such an $N$
must exist because $\sum P(E^{p, q} \text{ starts with } t_i) = 1$).

Now, each $P(E^{p, q} \text{ starts with } t_i)$
is continuous by assumption of continuous experiments, so the sum
$\sum\limits_{i < N} u_i P(E^{p, q} \text{ starts with } t_i)$
is also continuous.

Pick $\delta$
so that if $(p', q') \in B((p, q), \delta)$
then this sum changes by no more than $\frac{1}{3}\epsilon$.

But then the difference in $f_c$ can be no more than $\epsilon$:
The initial sum changes by no more than $\frac{1}{3}\epsilon$,
which means the tail can change by no more than $\frac{1}{3}\epsilon$,
which in turn means that the new tail has magnitude at most $\frac{2}{3}\epsilon$.
Thus the value of $c$ can only have changed by
$\frac{1}{3}\epsilon + \frac{2}{3}\epsilon = \epsilon$.
$p, q, \epsilon$
were all arbitrary, thus we have proved that $f_c$ is continuous.
\end{proof}

No non-trivial VNM rational agent can satisfy this criterion, because
for a VNM rational agent the only valid probabilities this can take on either
side of the indifference boundary are $0$ and $1$. The decision boundary is in the
closure of each half, so the only values it can take on the boundary are $0$
and $1$. Therefore by continuity any VNM rational agent with this property must
assign one outcome with probability $1$ everywhere and so is trivial.

Sampling models hint at an interesting alternate basis for modelling
rational behaviour mediated by experiments, suggesting that a better
model is that rather than considering discrete decision functions
we instead consider a random variable choosing between the two with
some probability. The theory then becomes more analogous to that
of multi-armed bandits than of classic VNM rationality.

I leave the question of how the two relate for future research.

\section{Non-Physical Agents}
It is worth exploring how much the ``natural'' criterion restricts the
set of agents that are available to us, and how weakening it could
permit more agents.

We'll find that even with much weaker conditions,  the ability of even
quite powerful sets of experiments to represent utility functions is quite limited,
but that with unnatural experiments we can at least define some rational agents.

In particular, we can model utility functions from any countable set just fine:

\begin{example}
For any countable set of utility functions \(\nu_i\) (e.g. all utility functions
with integer valued weights) define an experimental model for $\nu_i$
as follows:

Let \(\mathcal{E}_i = \mathcal{D}\), and let $E_i$ deterministically return the
correct answer for the current lotteries under $\nu_i$.

Given such an experiment, the agent that just returns $E_i$ correctly models $\nu_i$.
\end{example}

This is the best we can do:

\begin{theorem}
Any experimental model has agents correctly modelling at most countably
many inequivalent utility functions. 
\end{theorem}

\begin{proof}
The idea of this proof is to use the linear structure of utility functions
acting on lotteries as follows. We'll show that you can reconstruct the
set of lotteries to which a utility function is indifferent from its
values on only finitely many values drawn from traces of agents representing
it. There are only countably many possible traces, and therefore only
countably many possible sets of indifference which can be
represented by agents.

But if two utility functions represent the same set of indifference, they
are either equivalent or opposite. Therefore there are are at most
countably many inequivalent utility functions represented by agents.

We'll now show how to recover the set of indifference from an agent
representing the utility function.

A non-constant utility function $\nu$ defines a linear functional
$\lambda: \mathbb{R}^N \to \mathbb{R}$ by $\lambda(x) = \sum \nu_i x_i$.

$k_\nu$ is indifferent between $p$ and $q$ whenever $\lambda(p - q) = 0$.

But this kernel forms a linear subspace of $\mathbb{R}^N$ of codimension $1$.

So we can find linearly independent vectors $u_1, \ldots, u_{N-1}$ which span
the space. By picking some non-zero element of the kernel and subtracting a
multiple of it from each we can assume that $\sum\limits_j (u_i)_j = 0$ for
all $i$.

We can now use the $u_i$ to construct $N - 1$ lotteries $p_i, q_i$ between
which $d_\nu$ is indifferent such that the $p_i - q_i$ are all linearly
independent.

To do this, simply define $p_i = t + \frac{\epsilon} u_i$,
$q_i = t - \frac{\epsilon} u_i$, where $t$ is the uniform vector
$(\frac{1}{N}, \ldots, \frac{1}{N})$, and $\epsilon > 0$ is any
sufficiently small number to keep all the coordinates within the bounds
$[0, 1]$.

Then the differences are $p_i - q_i = 2\epsilon u_i$, so these span the
kernel as desired.

We can now use this construction to label any VNM rational agent.

Let $d$ be such an agent for utility function $\nu$.

We know from the above that there exist $N - 1$ pairs of lotteries $(p_i, q_i)$
whose differences are linearly independent and such that
$P(d(E^{p_i, q_i} = I) = 1)$.

Pick any such $p_i, q_i$ and let $t_i$ be a trace that occurs in $E^{p_i, q_i}$
with non-zero probability.

Then the set $\{t_i\}$ uniquely identifies the set of lotteries to which
$\nu$ can be indifferent, because it must be indifferent to each of the $p_i, q_i$,
as $t_i$ occurs there with non-zero probability so if $t_i$ is present
in its set then it must return indifference with non-zero probability and
thus probability $1$ (because a rational agent always returns the same
answer with probability $1$).

But therefore given such a set the $p_i - q_i$ are a set of $N - 1$ linearly
independent vectors in the kernel of $\lambda_\nu$, and so span the whole Kernel.

There are only countably many such sets (because the set of finite sequences is
countable, and the set of finite subsets of a countable set is countable), and
at most two inequivalent utility functions have the same set assigned to them,
so the set of equivalence classes of utility functions represented by agents
is at most countable.

\end{proof}

Note that despite this, experimental models will often have uncountably many
inequivalent agents:

Let $u \in \mathcal{D}^\mathbb{N}$
and consider the experimental setup where
$\mathcal{E}_i = \{0, 1\}$
and the $E_i$
are independent Bernoulli random
variables with parameter $\frac{1}{2}$.
Then the agent $\alpha_u$
that returns $u_i$
where $i$
is the first index with value $1$
is defined for every sequence of values except for the one that is
constantly zero, so it is defined with probability $1$
but if $u \neq v$
then for some $i$,
$u_i \neq v_i$
and so $\alpha_u(t) \neq \alpha_v(t)$
whenever $t$
starts with $i - 1$
$0s$
followed by a $1$
(which happens with probability $2^{-i} > 0$)
so the two agents are not equivalent.

This example also has another important property: There are only countably
many agents that are defined on all of $\mathcal{E}$,
so the definition of agents as \textit{partial} functions was a strict
improvement in generality:

\begin{theorem}
Let $F \subseteq \mathcal{E}$
be a closed set.

There are at most countably many distinct agents (up to equality on $F$)
which are fully defined on all of $F$.
\end{theorem}

\begin{proof}
Let $d$ be an agent which is defined for all of $F$.

For $t = (t_1, \ldots, t_n)$
a finite sequence let $A_t = \{ x \in F : x \text{ starts with } t\}$.

This is an open set in $A^{\mathbb{N}}$ with the product topology.

But the definition of continuity means that
$F = \bigcup \{A_t: d \text { is constant on } A_t \}$.

$\mathcal{E}$
is compact by Tychonoff's Theorem, so $F$
is a closed subset of a compact space and thus compact. 

This is an open cover of $F$
and thus by compactness has a finite subcover.

Let $n$ be some number longer than the longest $t$ in that subcover. Then for
any $x, y$ if $x_i = y_i$ for $i \leq n$, $x$ and $y$ must start with some
common $t$ from that subcover and thus must have the same value.

Therefore $d$ is entirely defined by the choice of $n$ and some function
$d_n: \mathcal{E}_1 \times \ldots \times \mathcal{E}_n \to \mathcal{D}$.
There are only countably many choices
of $n$ and only finitely many choices of $d_n$ given $n$, so the set of
possible agents is a countable union of finitely many sets and thus is
countable.
\end{proof}

The big difference between fully defined and partially defined agents that
comes into play here is that compactness forces fully defined agents to
inspect not merely a finite number of experimental outcomes but a \textit{bounded}
number. In our above examples the closer we came to the missing point the
more outcomes we had to examine to determine the answer. We would get
the same behaviour if the agent were not fully defined but merely
defined on any closed subset that occurs with probability $1$.

Now, lets see what happens when we introduce a very small amount of uncertainty.

In the experiments where we modelled utility functions, we got a three-way
split---we could tell which side of the boundary we were on, and we could
tell whether we were on the boundary.

What happens if we introduce a slightly shaky hand at the boundary? i.e.
when we're on a boundary value instead of always returning $I$ the experiment
sometimes returns $L$ or $R$. The agent that always returns the experimental
result is of course now only weakly rational, but can that indecision be
recovered from?

In a large class of cases, no:

\begin{theorem}
Suppose the experimental setup has the property that for any $t_1, \ldots, t_n$
the set $\{(p, q): P(E^{p, q}_1 = t_1, \ldots, E^{p, q}_n = t_n) > 0\}$
is closed.

Any agent defined on some closed set has some $p, q$ where it is non-deterministic.
i.e. has at least two values it can return with non-zero probability and thus is
not rational.
\end{theorem}

\begin{proof}
Let $d$ be such an agent. Then by the same compactness argument as for total
agents there is some $n$ such that $d$ only depends on the first $n$ values of its input.

Thus for any $c \in \mathcal{D}$,
the set $A_c = \{(p, q): P(d(E^{p, q}) = c) > 0\}$
is a union of at most $\prod\limits_{i \leq n} |\mathcal{E}_i|$
sets of the form $\{(p, q): P(E^{p, q}_1 = t_1, \ldots, E^{p, q}_n = t_n) > 0\}$
and therefore is a finite union of closed sets and so closed.

But then $\bigcup A_c$
is a cover of $\mathcal{L}^2$
by finitely many closed sets. $\mathcal{L}^2$
is connected, so these cannot be disjoint. i.e. there must be $c \neq c'$ 
with $A_c \cap A_{c'} \neq 0$.

Any $(p, q)$
in that intersection can be assigned the values $c$ and $c'$
by $d$
with non-zero probability, so is the desired point.
\end{proof}

An agent in such a scenario can still be \textit{weakly} rational of course:
Just take the example where the experiments are utility functions and add some
small amount of uncertainty at the boundary.

Although the fact that this only applies to total agents may give some hope,
for many powerful classes of experiment all agents must be total:

Another example of such a model is one where we can pick a binary representation for
each $p_i, q_i$
uniformly at random from the possibilities (i.e. deterministically for anything
that isn't a dyadic rational and pick one of the two representations for each
dyadic rational with equal probability) and then $E^{p, q}_i$ reads off the
corresponding $2^{2N}$
bits.

Every possibly input sequence corresponds to some element of $(\mathbb{R^N})^2$,
and the set which corresponds to an element in $\mathcal{L}^2$
is a closed subset (because it's the continuous preimage of a closed subset).

Moreover, every value in that subset appears with non-zero probability for some
lottery: Every $(p, q)$
has at most $2^k$
possible values in $E^{p, q}$,
where $k$
is the number of coordinates which are dyadic rationals (of the form $\frac{m}{2^n}$),
which it picks from uniformly, so every point appears with non-zero probability
in its corresponding lottery, and thus every agent must be defined on the whole
set.

That is, we can measure where we are in the space with arbitrary precision and only
a small amount of uncertainty where we are precisely on the boundary, but we
still can't get rationality perfectly right and will sometimes declare a preference
where we should be indifferent (and possibly vice versa).

So although our naturality condition proved to be a reasonably strong restriction,
even without it our options are quite limited: Without experiments designed with
a priori knowledge of the agent's possible preferences, most rational agents
cannot operate perfectly in an experimental setup, and even a small amount of
uncertainty prevents agents from being more than weakly rational.

Some open questions remain:

\begin{itemize}
\item Is there a set of experiments that permits uncountably many utility functions
to be weakly modelled?
\item Is there an example of a set of experiments such that the sets
$\{(p, q): P(E^{p, q}_1 = t_1, \ldots, E^{p, q}_n = t_n) > 0\}$
are closed, but there is nevertheless a rational agent? (by the above theorem
such an agent cannot be defined on a closed set).
\end{itemize}

I regard these primarily as curiosities due to their dependence on unnatural experiments. 

\section{Conclusion}

I  have shown that agents under physically plausible assumptions, even logically
omniscient agents cannot behave in a way that appears to be rational to an
outside observer with perfect information.

Even with the ability to perform unnatural experiments which give far more
knowledge about the world than one might reasonably expect for more scenarios,
the ability to behave rationally is still somewhat limited.

Although this does not limit the behaviour of rational agents with perfect
a priori knowledge about the consequences of their choices, or the ability of
the Von Neumann-Morgenstern theorem to help discover the preferences of such,
it does suggest that expected utility maximization is not a good model of how
even logically omniscient agents should reason about the world, and suggest
that we might want to seek other models.

\bibliography{references}{}
\bibliographystyle{acm}

\end{document}
