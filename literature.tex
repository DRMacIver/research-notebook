\input{./includes/preamble.tex}

\title{PhD Report}
\author{David R. MacIver}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}\label{chap:introduction}

NB:\ This literature review is very much a work in progress and is not really ready for public consumption.
It is and will be highly fragmented,
with large missing gaps and much of the bits that are here only in a temporary state that will later be reworked into other things.
Proceed with caution.

Let me begin with some context.

The focus of my PhD is an open source project I wrote (and that many others now also contribute to) called \emph{Hypothesis}.
Hypothesis was originally purely a Python library\footnote{\url{https://github.com/HypothesisWorks/hypothesis-python}},
but now also has a version for Ruby\footnote{\url{https://github.com/HypothesisWorks/hypothesis-ruby}},
with a Rust backend that will eventually form a common implementation for the two.
I will blur the distinction and simply refer to them all as ``Hypothesis''.

Hypothesis is a library in the spirit of QuickCheck\cite{DBLP:conf/icfp/ClaessenH00} for \emph{property-based testing}.
The distinguishing feature of such tools is that they make it easy for users to define tests that combine user-written specifications with some form of data generation.
Such tests assert properties that are intended to hold for an entire class of inputs,
although in practice only ever test on a finite number of inputs in any given run.

I do not propose to do an extensive review of the existing literature on QuickCheck,
for two main reasons:

The first is that it would frankly be redundant.
If you are interested in a detailed survey of QuickCheck and QuickCheck-related tools in the context of Haskell,
I refer you to Rudy Braquehais's recent PhD thesis\cite{matela2017tools},
where he ably summarizes it and a wide variety of related tools.

The second is that despite the shared lineage,
due to differing implementation strategies and contexts,
Hypothesis actually has comparably little in common with QuickCheck that they do not both share in common with a much broader area of testing research.

QuickCheck nevertheless provides the starting point for many of the \emph{problems} that I will motivate,
and where relevant I will of course first back to it and its descendants.

A test in QuickCheck looks something like the following:

\begin{lstlisting}[language=Haskell]
import Test.QuickCheck (quickCheck, (==>), Property)
import Data.List (elem, delete)

prop_remove :: [Int] -> Int -> Property
prop_remove ls x = elem x ls ==> not (elem x (delete x ls))

main = quickCheck prop_remove
\end{lstlisting}

In this test we test the property that given a list of integers and an integer,
with the integer contained in the list,
after removing the integer from the list it is not longer present.

A similar property expressed in Hypothesis would be:

\begin{lstlisting}[language=Python]
from hypothesis import given, assume
import hypothesis.strategies as st


@given(st.lists(st.integers()), st.integers())
def test_deleting(ls, i):
    assume(i in ls)
    ls.remove(i)
    assert i not in ls
\end{lstlisting}

When run, these tests both do the same thing:
They randomly generate a list of integers and an integer,
if the integer is contained in the list they continue the test,
otherwise they mark it as invalid.
Once they have found a sufficient number of valid test cases that did not fail (100 by default) the test stops.
If prior to that point they have found a counter-example to the property,
they begin a process of test-case reduction to attempt to produce a smaller, simpler, example.

QuickCheck and its descendants,
including Hypothesis,
are essentially just libraries designed to make it easier to write tests that do this,
by providing some test running infrastructure and a library of combinators\footnote{
In the original Haskell QuickCheck these are typically expressed using type classes rather than explicit combinators,
but the Erlang version written by the same authors uses combinators,
as do many other ports.
Recently Hedgehog\footnote{\url{https://en.wikipedia.org/wiki/QuickCheck}} has introduced the combinator style in Haskell as well.
}
for specifying how to generate test cases.

From a research point of view,
this is an extremely boring idea,
and the QuickCheck authors knew that when they proposed it.
The following quote is from~\cite{DBLP:conf/icfp/ClaessenH00}:

\begin{quote}
We have taken two relatively old ideas, namely specifications
as oracles and random testing, and found ways to make
them easily available to Haskell programmers.
\end{quote}

QuickCheck was never an attempt to push back the boundaries of testing research,
but instead a paper about tool usability.
The fundamental question posed by the paper is not ``can we test software better than we are currently doing?'',
but ``how do we get non-researchers to use better testing techniques?''.

QuickCheck's answer to this question has proven fairly popular.
Most programming languages have a library inspired by it (often several)\footnote{\url{https://en.wikipedia.org/wiki/QuickCheck}},
and many of these libraries are widely used.
\href{http://libraries.io}{libraries.io} currently (as checked on 2018--04--29) lists 132 open source repositories using QuickCheck\footnote{\url{https://libraries.io/hackage/QuickCheck/usage}} and 532 using Hypothesis\footnote{\url{https://libraries.io/pypi/hypothesis/usage}}.
Other popular implementations that libraries.io did not seem to have good data for include test.check\footnote{\url{https://github.com/clojure/test.check/}} for Clojure and ScalaCheck\footnote{\url{http://www.scalacheck.org/}} for Scala.

I am currently focused on answering two follow-on questions:

\begin{itemize}
\item Given that usability is the main raison d'Ãªtre of QuickCheck style testing,
how can we improve that further?
\item Can we extend this sort of tooling beyond random testing which,
while surprisingly effective in practice,
ultimately limits the effectiveness of the resulting tests for more intensive testing?
\end{itemize}

Although the first question seems more a matter of engineering than research,
as I shall discuss in Chapter~\ref{TODO} there are some interesting research questions implied by it.

\chapter{A Note on Economics}

One interesting aspect of QuickCheck that makes it unusual among software from testing research is that it is in many ways much more aligned with the needs of the users than the needs of researchers.

Any given testing project has essentially two places where time needs to be invested:

\begin{itemize}
\item Time invested in understanding \emph{the general problem of testing}.
\item Time invested in understanding \emph{the behaviour of the specific system under test}.
\end{itemize}

Researchers in software testing differ from users of testing tools in terms of which one of these is ``their job''---a
researcher will happily invest a great deal of time into understanding the general problem of testing,
but unless they are focused on a very specific problem domain (such as testing C compilers,
of which there are only a few widely used open source ones,
and all of which share a common specification and similar patterns of behaviour),
they will typically want to apply their tools to \emph{many} systems under test,
with relatively little interst in investing time on each one.

In contrast,
a practitioner has typically already invested significant time into understanding the system they are testing,
and if they have not already they will do so (possibly through the process of testing it!),
but even if they are interested in the general problem of testing they probably do not have a great deal of time to spend on it.

One way in which this comes up,
and where QuickCheck differs significantly from many other approaches,
is the \emph{oracle problem}.
The oracle problem,
colloquially,
is that you can't know whether a piece of software is correct unless you know what the software is supposed to do.
Thus every test consists (possibly implicitly) of two parts:

\begin{itemize}
\item A \emph{test case} consisting of a set of inputs which exercises the system under test (SUT) in some way.
\item A \emph{test oracle} which can determine whether the SUT behaved incorrectly on those inputs.
\end{itemize}

Typically test oracles are sound but not complete---that is,
if the test oracle says that the behaviour is \emph{incorrect} then the behaviour is actually incorrect,
but the behaviour may be incorrect in some way that the test oracle is unable to detect.

The current research on identifying test oracles is well described in~\cite{DBLP:journals/tse/BarrHMSY15},
but to summarize along slightly different axes than they do,
the following are the main ways in which researchers currently look for test oracles:

\begin{itemize}
\item They use test oracles that only look for behaviour that is \emph{obviously wrong}.
e.g.\ segfaults, assertion failures, and deadlocks.
\item They derive a test oracle (either automatically or manually) from some specification of the behaviour of the software.
\item They use multiple implementations of the same specification to do differential testing---comparing
the implementations and seeing if they produce the same answer.
\item They attempt to derive the oracle from the current behaviour of the SUT,
effectively producing a regression test suite---this
can't determine whether the current behaviour is correct,
but allows you to determine when it changes.
\end{itemize}

Practitioners on the other hand adopt a quite different approach to finding test oracles:
They just write them,
typically in the host language,
as part of their normal testing process.
QuickCheck simply embraces this,
which I believe is a key feature of its success---practitioners
can take their existing knowledge of how to test their software to write test oracles,
while using QuickCheck to simplify the process of test-case generation,
which is less their area of expertise.

\chapter{Test Case Generation}\label{chap:generation}

Test-case generation is,
unsurprisingly,
the process of producing a test case,
typically for some specific system under test and test oracle for it.

\chapter{Operations on Test Cases}

As discussed in Chapters~\ref{chap:introduction} and~\ref{chap:generation},
the key functionality of QuickCheck is that it provides a composable library of functions for specifying how to generate a test case.

There are many natural operations on test cases beyond generating them.
For example,
given a test case we might want to:

\begin{itemize}
\item Serialize it into some binary format suitable for use as a regression test.
\item Transform it into one that is easier to understand,
through a process of \emph{shrinking}.
I will discuss this in Section~\ref{sec:shrinking}.
\item \emph{Mutate} it into a (not necessarily simpler) similar test-case,
in order to better explore the search space. I will discuss this in Section~\ref{sec:fuzzing}.
\end{itemize}

It is possible in principle to do many of these operations fully generically based on the underlying representation of the values (e.g.~they could be algebraic data types,
or the language could use some common object model).
However this many violate abstraction boundaries---if
e.g.\ a data type is designed to represent a collection of values as a balanced binary tree,
manipulating its tree structure in a way that is unaware of that will produce an instance of the type that could never have been produced via its public API.\ 



\section{Test-Case Reduction}\label{sec:shrinking}

As well as test-case \emph{generation},
and important part of QuickCheck's utility comes from test-case \emph{reduction}.

Test-case reduction is the process of taking a failing test case and turning it into one that is in some sense ``smaller''.
Classically the notion of size for this problem is obvious---e.g.
the classic approach is delta debugging\cite{DBLP:journals/tse/ZellerH02},
which operates on arbitrary sequences and just uses the sequence length as its notion of size.

Closely related is the notion of test-case normalization\cite{DBLP:conf/issta/GroceHK17},
which attempts to to reduce ``semantic complexity'',
making the test case simpler even when it does not reduce the size,
by attempting to find a single canonical example of any given predicate.

Although the initial version of QuickCheck did not come with test-case reduction,
it was proposed by a user even before the paper was published:

\begin{quote}
He [Andy Gill, a user of the library] made an improvement in the way QuickCheck reports counter examples.
Soemtimes, the counter examples found are very large,
and it is difficult to go back to the property and understand why it is a counter example.
However,
when the counter example is an element of a tree shaped datatype,
the problem can often be located in one of the sub-trees of the counter example found.
Gill extended the arbitrary class with a new method

\begin{lstlisting}[language=Haskell]
smaller :: a -> [a]
\end{lstlisting}

which is intended to return a list of smaller, but similar values to its argument---for example, direct subtrees.
He adapted the \texttt{quickCheck} function so that when a counter example is found,
it tries to find a smaller one using this function.
In some cases much smaller counterexamples were found,
greatly reducing the time to understand the bug found.
\end{quote}

In modern QuickCheck this function is instead called \texttt{shrink},
and the combined process of test-case reduction and normalization is referred to as \emph{shrinking}.
I will use this terminology from now on except when referring to non-QuickCheck work.

Shrinking has a fairly essential role in making QuickCheck useful---as indeed observed in the paper---as randomly generated counter-examples can be large and hard to read.
Simple test-case reduction methods have proven highly effective at reducing these.
e.g.~ \cite{DBLP:conf/issre/LeiA05} reported that delta-debugging typically reduced test case sizes for randomly generated unit tests by an average of between 71\% and 93\% (depending on the unit being tested). 

Unfortunately the approach as implemented in QuickCheck has some significant limitations,
which work against the goal of providing easy to use combinators for specifying test-case generation.

The root of the issue is that while random generation composes easily,
naive implementations of test-case reduction do not.

Data generation naturally has a monadic structure:
You can chain generated values together to produce new generated values.
Test-case reduction when implemented as an explicit reducer function does not have this property.

In order to turn a reducer for one type into a reducer for another,
you need functions mapping the type in both directions.
Given a reducer \(\texttt{smaller} :: a \to [a]\) and a function \(a \to b\) (or a function \(b \to a\)) there is no way to produce a non-trivial reducer \(b \to [b]\)---either
without both you lack one of the ingredients you need to synthesize a suitable \(b\).

Two approaches have been approached to fix this which would allow the natural monadic structure to retain test-case reduction:

\begin{enumerate}
\item In SmartCheck\cite{DBLP:conf/haskell/Pike14},
test-case reduction is fully generic and works for arbitrary algebraic data types.
QuickCheck offers similar,
more limited,
functionality.
\item In the rose-tree based approach proposed by Reid Draper in~\cite{FreeShrinking},
rather than generating a single value,
the library generates a lazy tree of values,
with each node in the tree labelled by a single value,
and children corresponding to simpler values.
\end{enumerate}

\chapter{Goals}

TODO:\ This chapter needs a better title.

The primary goal of my research can be framed as follows:
How can we create tools that are as easy for practitioners to use 

\chapter{Unincorporated Material}

This chapter gathers papers that I must/should/could include in my review,
along with some notes on them,
but does not attempt to do any significant amount of synthesis.

\section{Must Haves}

\subsection{QuickCheck Papers}

The foundational paper for my work is of course ``QuickCheck: a lightweight tool for random testing of Haskell programs''\cite{DBLP:conf/icfp/ClaessenH00}.

There are a number of other papers worth referencing:

\begin{itemize}
\item ``Testing telecoms software with quviq QuickCheck''\cite{DBLP:conf/erlang/ArtsHJW06}
\item ``SmartCheck: automatic and efficient counterexample reduction and generalization''\cite{DBLP:conf/haskell/Pike14} is one of the few papers about improving QuickCheck's test case reduction,
so definitely worth including.
\end{itemize}

\subsection{Test Case Reduction Papers}

\begin{itemize}
\item ``Simplifying failure-inducing input''\cite{DBLP:conf/issta/HildebrandtZ00} is the delta-debugging paper that you have to cite if you ever write anything about test-case reduction.
\item ``One test to rule them all''\cite{DBLP:conf/issta/GroceHK17} is particularly relevant to Hypothesis because of the relationship between normalization and its shortlex-minimization goal.
\item ``Minimization of Randomized Unit Test Cases''\cite{DBLP:conf/issre/LeiA05} is a good justification for the combination of random test case generation and test-case reduction.
\end{itemize}


\section{Should Haves}

\begin{itemize}
\item ``Targeted property-based testing''\cite{DBLP:conf/issta/LoscherS17} is about how you can do what is basically hill climbing with restart to better test properties.
This is relevant mostly as an example of why it's nice to be able to extend with new operations on generated data.
\item ``Why is random testing effective for partition tolerance bugs?''\cite{DBLP:journals/pacmpl/MajumdarN18} is an interesting argument that random testing can and should work.
\item ``Partition Testing Does Not Inspire Confidence``\cite{DBLP:journals/tse/HamletT90} is a good argument that random testing is pretty OK.\ 
\item ``Behind Human Error''\cite{BehindHumanError} contains a lot of lucid discussion about error that I think is valuable for this sort of work.
\item ``An Experimental Evaluation of the Assumption of Independence in Multiversion Programming''\cite{DBLP:journals/tse/KnightL86}---a
lot of property-based testing is basically multiversion programming to do differential testing.
\item ``Software Testing Research: Achievements, Challenges, Dreams''\cite{DBLP:conf/icse/Bertolino07} (for a good survey of where software testing research is and wants to go, and some useful basic questions).
\end{itemize}

\section{Could Haves}

\begin{itemize}
\item ``Can a Machine Design?''\cite{doi:10.1162/07479360152681083} contains a really nice account of different modes of human/computer codesign,
and how having the computer make things for a human to correct is much less stressful than having the computer correct the human.
\item ``When and what to automate in software testing? {A} multi-vocal literature review''\cite{DBLP:journals/infsof/GarousiM16}---this
seems a natural fit.
\end{itemize}

\section{Really Want An Excuse To Haves}

As James Mickens puts it\cite{mickens2014saddest}:

\begin{quote}
``How can you make a reliable computer service?'' the presenter will ask in an innocent voice before continuing,
``It may be difficult if you canât trust anything and the entire concept of happiness is a lie designed by unseen overlords of endless deceptive power.''
The presenter never explicitly says that last part,
but everybody understands what's happening.
\end{quote}

\input{./includes/bibsection.tex}

\end{document}
