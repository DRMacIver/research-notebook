\input{./includes/preamble.tex}

\title{Literature Review for PhD}
\author{David R. MacIver}

\begin{document}

\maketitle

\tableofcontents

\chapter*{Preface}

This is the beginnings of the literature review and framing sections for my PhD Thesis.
It's currently in a considerably rougher state than I'd like,
so proceed with some caution.

As well as a bunch of missing content,
there are definitely going to be a few places where I've made some huge leaps of logic or forgot to introduce terminology.

Sorry.

\chapter{Background}\label{chap:introduction}

My work is concerned with the process of \emph{testing} software.
Testing consists of a series of tests,
each of which typically returns pass or fail,
with a fail indicating some defect in the software (or in the testing process)\footnote{
This terminology and usage is in fact somewhat controversial,
and the software testing/QA community of practitioners would often refer to this as \emph{checking},
and the individual steps as \emph{checks}.
James Bach presents an enlightening article discussing these terms at \url{http://www.satisfice.com/blog/archives/856}.
However,
the use of the term ``test'' is significantly more common in the research community,
and I will use this terminology.
}.

A \emph{test} can be broken up into three parts:

\begin{enumerate}
\item A \emph{system under test} (SUT)---some software that the testing process is targetting.
This might be an entire application (or indeed multiple interacting applications),
or it might be a single function,
or anywhere in between.
\item A \emph{test case}---some
inputs that are designed to exercise the system under test.
\item A \emph{test oracle}---a
procedure that observes the behaviour of the SUT when executed with the test case and returns pass or fail\footnote{
As we will see later, sometimes there is a third possible return value that indicates that the test case is in some way invalid.
For the purpose of this section we can consider that equivalent to a pass.
}.
\end{enumerate}

Often the boundaries between these are blurred.
e.g.\ the distinction between test case and oracle is not always obvious in tests,
and with e.g.\ the introduction of mocking or mutation testing even the boundary between SUT and test case is not always clear.
Nevertheless the logical distinction between the three parts is useful.

Usually the SUT is fixed---we are interested in testing a particular piece of software,
not all the software in the world---with
maybe some freedom to select subsets of it (e.g.\ picking a particular function in a larger application to test).
The problem of how to test software then becomes the selection of suitable test cases and test oracles.

Typically software practitioners use a mostly manual process for selectng test cases and oracles---in
fully manual testing,
both the test case and the oracle are literally a ``human in the loop'' using the software,
but even with most automated testing the oracle and test case are written by hand by a developer of the software.
Executing the test case is then automated and repeatble, usually with some test runner such as JUnit\footnote{\url{https://junit.org/junit5/}} or pytest\footnote{\url{https://pytest.org/}},
but a human had to write it at some point.

However both the process of test-case generation and oracle discovery can be partially automated.
The following two articles are excellent surveys of the current research on doing so:

\begin{itemize}
\item ``The Oracle Problem in Software Testing: {A} Survey''\cite{DBLP:journals/tse/BarrHMSY15}
\item ``An orchestrated survey of methodologies for automated software test case generation''\cite{DBLP:journals/jss/AnandBCCCGHHMOE13}
\end{itemize}

I won't attempt to summarize the research on test case generation here,
as my focus is on a fairly specific approach within that,
but can recommend the survey paper for a broader picture.

From a research point of view,
the problem of test case generation is much more interesting and as such more well studied.
The oracle problem (i.e.\ finding a suitable test oracle) is in some sense impossible to solve in generality---it
is impossible to know whether the software's behaviour is correct unless you know what it is that the software is actually supposed to \emph{do}.

Within some areas such as compiler testing\cite{DBLP:conf/pldi/YangCER11, DBLP:conf/icse/DonaldsonL16},
the problem domain is both well enough specified and of sufficient research interest that this is not a problem---the
researchers testing compilers understand the compilers they are testing well enough to write useful oracles---but
for testing research in general this means having to put in a lot of boring and non-research work to understand the behaviour of the particular SUT.\ 

This means that most oracles studied in software testing research fall into one of the following categories:

\begin{itemize}
\item They derive a test oracle (either automatically or manually) from some specification of the behaviour of the software.
\item They use test oracles that only look for behaviour that is \emph{obviously wrong}.
e.g.\ segfaults, assertion failures, and deadlocks.
\item They use multiple implementations of the same specification to do differential testing---comparing
the implementations and seeing if they produce the same answer.
\item They attempt to derive the oracle from the current behaviour of the SUT,
effectively producing a regression test suite---this
can't determine whether the current behaviour is correct,
but allows you to determine when it changes.
\end{itemize}

All of these are of somewhat limited applicability in practice---most
software as developed has no formal specification or alternate implementation,
changes in behaviour of the software are often the point,
and crashes and deadlocks while frustrating are in some sense the least interesting category of bug because they are so obvious when they happen in production---catching
wrong behaviour that does \emph{not} lead to a crash is much higher value proposition.

In contrast,
the practitioner approach of just writing down a test oracle programmatically seems to work very well---the
developers of the software already \emph{know} what it is supposed to do (more or less),
and are experienced at writing test oracles as part of their own testing.

Unfortunately,
this runs into the other fork of the problem---practitioners
are experienced at writing test oracles,
but have little experience in writing test case generators.
They may be willing to acquire the basics,
but given the relative lack of widespread deployment of even basic test case generation,
it seems likely that it would require more than simple knowledge is required to improve the situation.

This is where the QuickCheck\cite{DBLP:conf/icfp/ClaessenH00} family of tools comes in to the picture.

QuickCheck is a software testing library,
originally for Haskell but with a large number of ports to other languages\footnote{
\url{https://hypothesis.works/articles/quickcheck-in-every-language/} curates a list of what I think are the ones that are actually good enough to be worth using,
but \url{https://en.wikipedia.org/wiki/QuickCheck} lists many more.
}.
A test in QuickCheck looks as follows:

\begin{lstlisting}[language=Haskell]
import Test.QuickCheck (quickCheck, (==>), Property)
import Data.List (elem, delete)

prop_remove :: [Int] -> Int -> Property
prop_remove ls x = elem x ls ==> not (elem x (delete x ls))

main = quickCheck prop_remove
\end{lstlisting}

In this test we test the property that given a list of integers and an integer,
with the integer contained in the list,
after removing the integer from the list it is not longer present (this property is false---the
remove function only removes the first element if there are duplicates).

As seeen in this example,
in the original QuickCheck the test case is specified entirely through its type,
using Haskell's type class mechanism to turn it into a test case generator.
In most descendants of QuickCheck instead explicit combinators to allow more fine grained control of the generation.
Since the recent development of Hedgehog\footnote{\url{https://github.com/hedgehogqa/haskell-hedgehog}},
this approach is also available in Haskell.

When run, this test randomly generates a number of examples (100 by default).
If none are rejected by the test oracle (the function provided by the user),
the test passes.
If it finds a test case which makes the test oracle fail it then performs test-case reduction\cite{DBLP:conf/issta/HildebrandtZ00, DBLP:conf/pldi/RegehrCCEEY12},
which attempts to turn the failing example into a simpler and easier to read one,
and shows the reduced example to the user.

This sort of testing is commonly referred to as \emph{property-based testing}\cite{DBLP:conf/erlang/ArtsHJW06}.

From the point of view of the core questions of testing research---test-case generation and identifying test oracles---QuickCheck
is a very boring piece of software (and this was mostly true even when it was created).
QuickCheck uses developer-written test oracles (in the original paper these are framed as ``specifications'',
but in actual use they are not substantially different from normal tests written by developers\footnote{\url{https://hypothesis.works/articles/incremental-property-based-testing/}}) and simple random generation of test cases.
While random testing is still a widely used technique in some domains
(e.g.\ it is still widespread compiler testing),
and is far more effective than one might suppose\cite{DBLP:journals/tse/HamletT90, DBLP:journals/pacmpl/MajumdarN18},
it was far from cutting edge even when the paper was published.

This is not intended as a criticism of the paper,
because the paper was never an attempt to push back the boundaries of testing research,
but instead to make the existing research more accessible!
The following quote is from the original QuickCheck paper\cite{DBLP:conf/icfp/ClaessenH00}:

\begin{quote}
We have taken two relatively old ideas, namely specifications
as oracles and random testing, and found ways to make
them easily available to Haskell programmers.
\end{quote}

This has been wildly successful,
even ignoring the various descendant libraries:
QuickCheck has over 1800 open source Haskell packages using it\footnote{\url{http://packdeps.haskellers.com/reverse/QuickCheck}},
and presumably has significantly more closed source or unpublished packages beyond that number.

Rudy Braquehais's recent PhD thesis\cite{matela2017tools} has an excellent summary of much of the current research on extending QuickCheck in Haskell.
I do not propose to cover much of this here,
as it does not have a great deal of overlap with my work---it
is largely focused on some Haskell specific problems (e.g.\ a lot of it is reliant on the widespread use of algebraic data types) and expanding QuickCheck with capabilities that go beyond testing per se
(e.g. FitSpec\cite{DBLP:conf/haskell/BraquehaisR16} expands QuickCheck with a form of mutation testing\cite{DBLP:journals/tse/JiaH11} to identify ``missing'' properties).
Where specific papers are relevant,
I will mention them in the appropriate context.

Instead of QuickCheck in its own right,
the focus of my PhD is an open source project I wrote (and that many others now also contribute to) called \emph{Hypothesis}.
Hypothesis was originally purely a Python library\footnote{\url{https://github.com/HypothesisWorks/hypothesis-python}},
but now also has a version for Ruby\footnote{\url{https://github.com/HypothesisWorks/hypothesis-ruby}},
with a Rust backend that will eventually form a common implementation for the two.
I will blur the distinction and simply refer to them all as ``Hypothesis''.

Hypothesis was originally a QuickCheck port,
but has mostly moved beyond that---it
shares its goals and has a roughly similar API,
but has a very different implementation.

A similar property to the above expressed in Hypothesis would be:

\begin{lstlisting}[language=Python]
from hypothesis import given, assume
import hypothesis.strategies as st


@given(st.lists(st.integers()), st.integers())
def test_deleting(ls, i):
    assume(i in ls)
    ls.remove(i)
    assert i not in ls
\end{lstlisting}

I am currently focused on answering two follow-on questions:

\begin{itemize}
\item Given that usability is the main raison d'être of QuickCheck style testing,
how can we improve that further to get wider spread adoption of these techniques?
\item Can we extend this sort of tooling with more advanced techniques than random testing,
without requiring users to understand or even be aware of those techniques?
\end{itemize}

Although the first question seems more a matter of engineering than research,
as I shall discuss in Chapter~\ref{TODO} there are some interesting research questions implied by it.

\chapter{Challenges in Property-Based Testing}

In this chapter I will discuss some of the challenges and limitations of existing solutions that I am seeking to overcome.
Specifically:

\begin{itemize}
\item QuickCheck and most of its descendants have a significant \emph{test-case validity problem}\cite{DBLP:conf/pldi/RegehrCCEEY12},
where many of the test cases it tries may be unsuitable for the provided oracle.
\item Existing attempts to 
\item In many cases,
QuickCheck requires a significant amount of expertise from the user to get high quality results.
\end{itemize}

\section{Test Case Validity in Property-Based Testing}\label{sec:validity}

The \emph{test-case validity problem}\cite{DBLP:conf/pldi/RegehrCCEEY12} is the issue that a given test oracle may only be suitable for some range of test cases.
The example in the context it was proposed is that Csmith tests C compilers by evaluating them on C programs that are free of undefined and implementation-defined behaviour,
then comparing the results across multiple compilers and seeing if they differ.
Csmith is able to guarantee that the programs it generates are valid for this test oracle,
but the property may be lost during test-case reduction as C-Reduce cannot guarantee that without using external checkers.

It's important to note from this that the inputs are invalid \emph{for the test case}.
A program that exhibits undefined behaviour is not an invalid input to the compiler (undefined behaviour is fundamentally a runtime property),
but the test oracle would return an incorrect answer when evaluated on it.

This problem crops up in much the same way in most property-based testing libraries.
For example,
consider the following test:

\begin{lstlisting}
from hypothesis import given, assume
import hypothesis.strategies as st

import math

@given(st.floats())
def test_sqrt_is_positive(x):
    assume(x >= 0)
    assert math.sqrt(x) >= 0
\end{lstlisting}

This test says that for any non-NaN non-negative floating point number,
its sqrt is positive.
This test correctly passes.

The \texttt{assume} function used specifies a precondition on the test that allows it to reject invalid inputs.
Hypothesis then essentially performs rejection sampling on the generated test cases,
halting the test whenveer an assumption is false.

The rejection sampling is not a major problem in this case,
because a test case is accepted with probability close to half,
but it degrades performance rapidly as you start to build more complicated test cases,
and also biases the distribution significantly in some cases.

For example,
suppose we had something like the following:

\begin{lstlisting}
from hypothesis import given, assume
import hypothesis.strategies as st

@given(st.lists(st.floats()))
def test_something_about_lists(xs):
    assume(all(x >= 0 for x in xs))
\end{lstlisting}

We are now making an assumption about each element of a list.
This means that we accept a list of length \(n\) with probability \(2^{-n}\).
As well as significantly degrading the performance of testing,
this also creates a very strong bias towards short lists on the test cases that we \emph{do} accept.

As such,
we frequently want to build these sorts of conditions into the generated data rather than the test.

For example we could write the first test instead as:

\begin{lstlisting}
from hypothesis import given, assume
import hypothesis.strategies as st

import math

@given(st.floats(min_value=0))
def test_sqrt_is_positive(x):
    assume(x >= 0)
    assert math.sqrt(x) >= 0
\end{lstlisting}

This will cause Hypothesis to use a generator that always produces non-negative floats,
avoiding the rejection sampling.

A natural question about this test however is whether we still need the \texttt{assume} call.
After all,
if the test-case generator always produces non-negative numbers,
the precondition seems entirely redundant.

Whether this is the case varies from library to library,
but it is \emph{not} redundant in QuickCheck (either the Haskell version or the later Erlang one).
In many implementations,
shrinking is based not on the generator but on the type of the generated value,
so shrinking may try values that could never have been generated.
In the above test,
the shrinker might for example try replacing some large generated float with the much simpler value `-1.0`,
which would be invalid for the test case.

This is not the case in Hypothesis,
where the shrinking is a built in part of the data generation at a very low level.
Additionally,
an earlier system which achieved similar effects was introduced by Reid Draper in~\cite{FreeShrinking},
and is in use in Clojure's test.check\footnote{\url{https://github.com/clojure/test.check}} and Hedgehog\footnote{\url{https://github.com/hedgehogqa/}}.
I will discuss the differences between the two systems and their trade-offs in the next section.

All else being equal,
having a single source of truth for what a valid test case looks like seems obviously better,
whether it is specified as preconditions or generators.
Aside from the redundant work,
having multiple sources of truth results in the two potentially getting out of sync.

Additionally,
in many cases specifying the full precondition for what a valid test-case looks like may be substantially harder than merely generating one!
e.g.\ while Csmith is a highly non-trivial project,
it's a lot simpler than the semantics checking interpreters that C-Reduce requires to maintain validity!

Thus,
in order to achieve this,
we want to ensure that the following principle holds:
Every \emph{reduced} test-case is one that could have been generated with non-zero probability.
This ensures that all properties that hold for every generated test-case also hold for every reduced test-case
(properties which merely hold with \emph{high} probability may of course still be violated by the reducer).

\section{Composing Test-Case Reduction}

Having discussed in Section~\ref{sec:validity} why we want a close relationship between test-case generation and reduction,
a natural approach is to somehow integrate the reduction into the generation process.
Unfortunately this leads to some difficulties.

A key property of random test-case generation that makes it so appealing is that is highly composable.
Specifically it has the structure of a \emph{monad}\footnote{
In QuickCheck it is only ``morally'' a monad,
as it technically violates the monad laws.
However this is easily fixed by changing the way it handles its random state.
} (in fact a monad with zero).
Without belabouring the category theoretic aspects of this,
what this means is that you can chain arbitrary data generators together---you
can generate a value,
do some more generation based on that,
and the whole result is still a perfectly normal test-case generator.
This property is what allows for the easy definition of a combinator library for defining test-case generators.

Unfortunately,
while test-case \emph{generation} has this property,
test-case reduction (and many other natural operations on test cases) do not share it.

The problem with composing test-case reducers as they are normally written---as
a local optimisation pass on generated values---is
that in order to transform one test-case reducer into another we need to know how to go in \emph{both} directions---you
have to transform the new value into the original format,
run the test-case reducer on it,
and transform it back.
The monadic interface only gives us one direction of this.

The solution proposed by Reid Draper\footnote{\url{https://mail.haskell.org/pipermail/libraries/2013-November/021674.html}} and implemented in test.check is to integrate shrinking into the generation process.
Instead of generating a single value,
an entire lazy tree of shrunk values is generated:

\begin{lstlisting}[language=Haskell]
data RoseTree a = RoseTree a [RoseTree a]

newtype Generator a = Generator { unGen :: StdGen -> Int -> a }

newtype Gen a = Gen (Generator (RoseTree a))
\end{lstlisting}

These can then be composed more straightforwardly.
e.g. you can compose a Generator a with a function \(f :: a \to b\) by applying \(f\) to the generated value at the root of the rose tree,
and recursively applying that to each of the children,
so that shrunk values on the composed generator are shrunk values of the original generator with \(f\) applied to them.

Unfortunately while this approach \emph{does} compose monadically,
it does not do so very well.

Suppose you have something like the following Hypothesis code:

\begin{lstlisting}[language=Python]
import hypothesis.strategies as st

sized_lists = st.integers(
  min_value=0, max_value=0
).flatmap(
  lambda n: st.lists(st.floats(), min_size=n, max_size=n)
)
\end{lstlisting}

In this we draw a length value between \(0\) and \(10\) and then draw a list of that size.

In this rose tree based approach the shrinks that are available to us are:

\begin{itemize}
\item Reduce the value of \(n\) and take the root of the generated rose tree for the reduced \(n\).
Typically this takes a prefix of the generated list.
\item Shrink individual values within the generated list.
\end{itemize}

The documentation for test.check includes an example much like this\footnote{\url{https://github.com/clojure/test.check/blob/master/doc/growth-and-shrinking.md\#unnecessary-bind}} and shows the problems that can arise:
Often you will end up with useless values at the beginning of the list that could easily have been deleted by a test-case reducer specialized to the problem,
but were unavailable to the rose tree approach because it is unable to make any shrinks that lower \(n\) and touch the generated values.

\input{./includes/bibsection.tex}
\end{document}
