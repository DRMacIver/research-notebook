\input{./includes/preamble.tex}

\title{PhD Report}
\author{David R. MacIver}

\begin{document}

\maketitle

\tableofcontents

\chapter{Background}\label{chap:introduction}

My work is concerned with the process of \emph{testing} software.
Testing consists of a series of tests,
each of which typically returns pass or fail,
with a fail indicating some defect in the software (or in the testing process)\footnote{
This terminology and usage is in fact somewhat controversial,
and the software testing/QA community of practitioners would often refer to this as \emph{checking},
and the individual steps as \emph{checks}.
James Bach presents an enlightening article discussing these terms at \url{http://www.satisfice.com/blog/archives/856}.
However,
the use of the term ``test'' is significantly more common in the research community,
and I will use this terminology.
}.

A \emph{test} can be broken up into three parts:

\begin{enumerate}
\item A \emph{system under test} (SUT)---some software that the testing process is targetting.
This might be an entire application (or indeed multiple interacting applications),
or it might be a single function,
or anywhere in between.
\item A \emph{test case}---some
inputs that are designed to exercise the system under test.
\item A \emph{test oracle}---a
procedure that observes the behaviour of the SUT when executed with the test case and returns pass or fail\footnote{
As we will see later, sometimes there is a third possible return value that indicates that the test case is in some way invalid.
For the purpose of this section we can consider that equivalent to a pass.
}.
\end{enumerate}

Often the boundaries between these are blurred.
e.g.\ the distinction between test case and oracle is not always obvious in tests,
and with e.g.\ the introduction of mocking or mutation testing even the boundary between SUT and test case is not always clear.
Nevertheless the logical distinction between the three parts is useful.

Usually the SUT is fixed---we are interested in testing a particular piece of software,
not all the software in the world---with
maybe some freedom to select subsets of it (e.g.\ picking a particular function in a larger application to test).
The problem of how to test software then becomes the selection of suitable test cases and test oracles.

Typically software practitioners use a mostly manual process for selectng test cases and oracles---in
fully manual testing,
both the test case and the oracle are literally a ``human in the loop'' using the software,
but even with most automated testing the oracle and test case are written by hand by a developer of the software.
Executing the test case is then automated and repeatble, usually with some test runner such as JUnit\footnote{\url{https://junit.org/junit5/}} or pytest\footnote{\url{https://pytest.org/}},
but a human had to write it at some point.

However both the process of test-case generation and oracle discovery can be partially automated.
The following two articles are excellent surveys of the current research on doing so:

\begin{itemize}
\item ``The Oracle Problem in Software Testing: {A} Survey''\cite{DBLP:journals/tse/BarrHMSY15}
\item ``An orchestrated survey of methodologies for automated software test case generation''\cite{DBLP:journals/jss/AnandBCCCGHHMOE13}
\end{itemize}

I won't attempt to summarize the research on test case generation here,
as my focus is on a fairly specific approach within that,
but can recommend the survey paper for a broader picture.

From a research point of view,
the problem of test case generation is much more interesting and as such more well studied.
The oracle problem (i.e.\ finding a suitable test oracle) is in some sense impossible to solve in generality---it
is impossible to know whether the software's behaviour is correct unless you know what it is that the software is actually supposed to \emph{do}.

Within some areas such as compiler testing\cite{DBLP:conf/pldi/YangCER11, DBLP:conf/icse/DonaldsonL16},
the problem domain is both well enough specified and of sufficient research interest that this is not a problem---the
researchers testing compilers understand the compilers they are testing well enough to write useful oracles---but
for testing research in general this means having to put in a lot of boring and non-research work to understand the behaviour of the particular SUT.\ 

This means that most oracles studied in software testing research fall into one of the following categories:

\begin{itemize}
\item They derive a test oracle (either automatically or manually) from some specification of the behaviour of the software.
\item They use test oracles that only look for behaviour that is \emph{obviously wrong}.
e.g.\ segfaults, assertion failures, and deadlocks.
\item They use multiple implementations of the same specification to do differential testing---comparing
the implementations and seeing if they produce the same answer.
\item They attempt to derive the oracle from the current behaviour of the SUT,
effectively producing a regression test suite---this
can't determine whether the current behaviour is correct,
but allows you to determine when it changes.
\end{itemize}

All of these are of somewhat limited applicability in practice---most
software as developed has no formal specification or alternate implementation,
changes in behaviour of the software are often the point,
and crashes and deadlocks while frustrating are in some sense the least interesting category of bug because they are so obvious when they happen in production---catching
wrong behaviour that does \emph{not} lead to a crash is much higher value proposition.

In contrast,
the practitioner approach of just writing down a test oracle programmatically seems to work very well---the
developers of the software already \emph{know} what it is supposed to do (more or less),
and are experienced at writing test oracles as part of their own testing.

Unfortunately,
this runs into the other fork of the problem---practitioners
are experienced at writing test oracles,
but have little experience in writing test case generators.
They may be willing to acquire the basics,
but given the relative lack of widespread deployment of even basic test case generation,
it seems likely that it would require more than simple knowledge is required to improve the situation.

This is where the QuickCheck\cite{DBLP:conf/icfp/ClaessenH00} family of tools comes in to the picture.

QuickCheck is a software testing library,
originally for Haskell but with a large number of ports to other languages\footnote{
\url{https://hypothesis.works/articles/quickcheck-in-every-language/} curates a list of what I think are the ones that are actually good enough to be worth using,
but \url{https://en.wikipedia.org/wiki/QuickCheck} lists many more.
}.
A test in QuickCheck looks as follows:

\begin{lstlisting}[language=Haskell]
import Test.QuickCheck (quickCheck, (==>), Property)
import Data.List (elem, delete)

prop_remove :: [Int] -> Int -> Property
prop_remove ls x = elem x ls ==> not (elem x (delete x ls))

main = quickCheck prop_remove
\end{lstlisting}

In this test we test the property that given a list of integers and an integer,
with the integer contained in the list,
after removing the integer from the list it is not longer present (this property is false---the
remove function only removes the first element if there are duplicates).

As seeen in this example,
in the original QuickCheck the test case is specified entirely through its type,
using Haskell's type class mechanism to turn it into a test case generator.
In most descendants of QuickCheck instead explicit combinators to allow more fine grained control of the generation.
Since the recent development of Hedgehog\footnote{\url{https://github.com/hedgehogqa/haskell-hedgehog}},
this approach is also available in Haskell.

When run, this test randomly generates a number of examples (100 by default).
If none are rejected by the test oracle (the function provided by the user),
the test passes.
If it finds a test case which makes the test oracle fail it then performs test-case reduction\cite{DBLP:conf/issta/HildebrandtZ00, DBLP:conf/pldi/RegehrCCEEY12},
which attempts to turn the failing example into a simpler and easier to read one,
and shows the reduced example to the user.

From the point of view of the core questions of testing research---test-case generation and identifying test oracles---QuickCheck
is a very boring piece of software (and this was mostly true even when it was created).
QuickCheck uses developer-written test oracles (in the original paper these are framed as ``specifications'',
but in actual use they are not substantially different from normal tests written by developers\footnote{\url{https://hypothesis.works/articles/incremental-property-based-testing/}}) and simple random generation of test cases.
While random testing is still a widely used technique in some domains
(e.g.\ it is still widespread compiler testing),
and is far more effective than one might suppose\cite{DBLP:journals/tse/HamletT90, DBLP:journals/pacmpl/MajumdarN18},
it was far from cutting edge even when the paper was published.

This is not intended as a criticism of the paper,
because the paper was never an attempt to push back the boundaries of testing research,
but instead to make the existing research more accessible!
The following quote is from the original QuickCheck paper\cite{DBLP:conf/icfp/ClaessenH00}:

\begin{quote}
We have taken two relatively old ideas, namely specifications
as oracles and random testing, and found ways to make
them easily available to Haskell programmers.
\end{quote}

This has been wildly successful,
even ignoring the various descendant libraries:
QuickCheck has over 1800 open source Haskell packages using it\footnote{\url{http://packdeps.haskellers.com/reverse/QuickCheck}},
and presumably has significantly more closed source or unpublished packages beyond that number.

Rudy Braquehais's recent PhD thesis\cite{matela2017tools} has an excellent summary of much of the current research on extending QuickCheck in Haskell.
I do not propose to cover much of this here,
as it does not have a great deal of overlap with my work---it
is largely focused on some Haskell specific problems (e.g.\ a lot of it is reliant on the widespread use of algebraic data types) and expanding QuickCheck with capabilities that go beyond testing per se
(e.g. FitSpec\cite{DBLP:conf/haskell/BraquehaisR16} expands QuickCheck with a form of mutation testing\cite{DBLP:journals/tse/JiaH11} to identify ``missing'' properties).
Where specific papers are relevant,
I will mention them in the appropriate context.

Instead of QuickCheck in its own right,
the focus of my PhD is an open source project I wrote (and that many others now also contribute to) called \emph{Hypothesis}.
Hypothesis was originally purely a Python library\footnote{\url{https://github.com/HypothesisWorks/hypothesis-python}},
but now also has a version for Ruby\footnote{\url{https://github.com/HypothesisWorks/hypothesis-ruby}},
with a Rust backend that will eventually form a common implementation for the two.
I will blur the distinction and simply refer to them all as ``Hypothesis''.

Hypothesis was originally a QuickCheck port,
but has mostly moved beyond that---it
shares its goals and has a roughly similar API,
but has a very different implementation.

A similar property to the above expressed in Hypothesis would be:

\begin{lstlisting}[language=Python]
from hypothesis import given, assume
import hypothesis.strategies as st


@given(st.lists(st.integers()), st.integers())
def test_deleting(ls, i):
    assume(i in ls)
    ls.remove(i)
    assert i not in ls
\end{lstlisting}

I am currently focused on answering two follow-on questions:

\begin{itemize}
\item Given that usability is the main raison d'être of QuickCheck style testing,
how can we improve that further to get wider spread adoption of these techniques?
\item Can we extend this sort of tooling with more advanced techniques than random testing,
without requiring users to understand or even be aware of those techniques?
\end{itemize}

Although the first question seems more a matter of engineering than research,
as I shall discuss in Chapter~\ref{TODO} there are some interesting research questions implied by it.

\chapter{Operations on Test Cases}

As discussed in Chapters~\ref{chap:introduction} and~\ref{chap:generation},
the key functionality of QuickCheck is that it provides a composable library of functions for specifying how to generate a test case.

There are many natural operations on test cases beyond generating them.
For example,
given a test case we might want to:

\begin{itemize}
\item Serialize it into some binary format suitable for use as a regression test.
\item Transform it into one that is easier to understand,
through a process of \emph{shrinking}.
I will discuss this in Section~\ref{sec:shrinking}.
\item \emph{Mutate} it into a (not necessarily simpler) similar test-case,
in order to better explore the search space. I will discuss this in Section~\ref{sec:fuzzing}.
\end{itemize}

It is possible in principle to do many of these operations fully generically based on the underlying representation of the values (e.g.~they could be algebraic data types,
or the language could use some common object model).
However this many violate abstraction boundaries---if
e.g.\ a data type is designed to represent a collection of values as a balanced binary tree,
manipulating its tree structure in a way that is unaware of that will produce an instance of the type that could never have been produced via its public API.\ 

\section{Generation}

Test-case generation is,
unsurprisingly,
the process of producing a test case for some specific system under test and test oracle for it.

Typically this starts from some description of what the range of test cases should be.
In the context of QuickCheck and similar tools,
this is specified programmatically,
but in other

\section{Test-Case Reduction}\label{sec:shrinking}

As well as test-case \emph{generation},
and important part of QuickCheck's utility comes from test-case \emph{reduction}.

Test-case reduction is the process of taking a failing test case and turning it into one that is in some sense ``smaller''.
Classically the notion of size for this problem is obvious---e.g.
the classic approach is delta debugging\cite{DBLP:journals/tse/ZellerH02},
which operates on arbitrary sequences and just uses the sequence length as its notion of size.

Closely related is the notion of test-case normalization\cite{DBLP:conf/issta/GroceHK17},
which attempts to to reduce ``semantic complexity'',
making the test case simpler even when it does not reduce the size,
by attempting to find a single canonical example of any given predicate.

Although the initial version of QuickCheck did not come with test-case reduction,
it was proposed by a user even before the paper was published:

\begin{quote}
He [Andy Gill, a user of the library] made an improvement in the way QuickCheck reports counter examples.
Soemtimes, the counter examples found are very large,
and it is difficult to go back to the property and understand why it is a counter example.
However,
when the counter example is an element of a tree shaped datatype,
the problem can often be located in one of the sub-trees of the counter example found.
Gill extended the arbitrary class with a new method

\begin{lstlisting}[language=Haskell]
smaller :: a -> [a]
\end{lstlisting}

which is intended to return a list of smaller, but similar values to its argument---for example, direct subtrees.
He adapted the \texttt{quickCheck} function so that when a counter example is found,
it tries to find a smaller one using this function.
In some cases much smaller counterexamples were found,
greatly reducing the time to understand the bug found.
\end{quote}

In modern QuickCheck this function is instead called \texttt{shrink},
and the combined process of test-case reduction and normalization is referred to as \emph{shrinking}.
I will use this terminology from now on except when referring to non-QuickCheck work.

Shrinking has a fairly essential role in making QuickCheck useful---as indeed observed in the paper---as randomly generated counter-examples can be large and hard to read.
Simple test-case reduction methods have proven highly effective at reducing these.
e.g.~ \cite{DBLP:conf/issre/LeiA05} reported that delta-debugging typically reduced test case sizes for randomly generated unit tests by an average of between 71\% and 93\% (depending on the unit being tested). 

Unfortunately the approach as implemented in QuickCheck has some significant limitations,
which work against the goal of providing easy to use combinators for specifying test-case generation.

The root of the issue is that while random generation composes easily,
naive implementations of test-case reduction do not.

Data generation naturally has a monadic structure:
You can chain generated values together to produce new generated values.
Test-case reduction when implemented as an explicit reducer function does not have this property.

In order to turn a reducer for one type into a reducer for another,
you need functions mapping the type in both directions.
Given a reducer \(\texttt{smaller} :: a \to [a]\) and a function \(a \to b\) (or a function \(b \to a\)) there is no way to produce a non-trivial reducer \(b \to [b]\)---either
without both you lack one of the ingredients you need to synthesize a suitable \(b\).

Two approaches have been approached to fix this which would allow the natural monadic structure to retain test-case reduction:

\begin{enumerate}
\item In SmartCheck\cite{DBLP:conf/haskell/Pike14},
test-case reduction is fully generic and works for arbitrary algebraic data types.
QuickCheck offers similar,
more limited,
functionality.
\item In the rose-tree based approach proposed by Reid Draper in~\cite{FreeShrinking},
rather than generating a single value,
the library generates a lazy tree of values,
with each node in the tree labelled by a single value,
and children corresponding to simpler values.
\end{enumerate}

\chapter{Goals}

TODO:\ This chapter needs a better title.

The primary goal of my research can be framed as follows:
How can we create tools that are as easy for practitioners to use 

\chapter{Unincorporated Material}

This chapter gathers papers that I must/should/could include in my review,
along with some notes on them,
but does not attempt to do any significant amount of synthesis.

\section{Must Haves}

\subsection{QuickCheck Papers}

The foundational paper for my work is of course ``QuickCheck: a lightweight tool for random testing of Haskell programs''\cite{DBLP:conf/icfp/ClaessenH00}.

There are a number of other papers worth referencing:

\begin{itemize}
\item ``Testing telecoms software with quviq QuickCheck''\cite{DBLP:conf/erlang/ArtsHJW06}
\item ``SmartCheck: automatic and efficient counterexample reduction and generalization''\cite{DBLP:conf/haskell/Pike14} is one of the few papers about improving QuickCheck's test case reduction,
so definitely worth including.
\end{itemize}

\subsection{Test Case Reduction Papers}

\begin{itemize}
\item ``Simplifying failure-inducing input''\cite{DBLP:conf/issta/HildebrandtZ00} is the delta-debugging paper that you have to cite if you ever write anything about test-case reduction.
\item ``One test to rule them all''\cite{DBLP:conf/issta/GroceHK17} is particularly relevant to Hypothesis because of the relationship between normalization and its shortlex-minimization goal.
\item ``Minimization of Randomized Unit Test Cases''\cite{DBLP:conf/issre/LeiA05} is a good justification for the combination of random test case generation and test-case reduction.
\end{itemize}


\section{Should Haves}

\begin{itemize}
\item ``Targeted property-based testing''\cite{DBLP:conf/issta/LoscherS17} is about how you can do what is basically hill climbing with restart to better test properties.
This is relevant mostly as an example of why it's nice to be able to extend with new operations on generated data.
\item ``Why is random testing effective for partition tolerance bugs?''\cite{DBLP:journals/pacmpl/MajumdarN18} is an interesting argument that random testing can and should work.
\item ``Partition Testing Does Not Inspire Confidence``\cite{DBLP:journals/tse/HamletT90} is a good argument that random testing is pretty OK.\ 
\item ``Behind Human Error''\cite{BehindHumanError} contains a lot of lucid discussion about error that I think is valuable for this sort of work.
\item ``An Experimental Evaluation of the Assumption of Independence in Multiversion Programming''\cite{DBLP:journals/tse/KnightL86}---a
lot of property-based testing is basically multiversion programming to do differential testing.
\item ``Software Testing Research: Achievements, Challenges, Dreams''\cite{DBLP:conf/icse/Bertolino07} (for a good survey of where software testing research is and wants to go, and some useful basic questions).
\end{itemize}

\section{Could Haves}

\begin{itemize}
\item ``Can a Machine Design?''\cite{doi:10.1162/07479360152681083} contains a really nice account of different modes of human/computer codesign,
and how having the computer make things for a human to correct is much less stressful than having the computer correct the human.
\item ``When and what to automate in software testing? {A} multi-vocal literature review''\cite{DBLP:journals/infsof/GarousiM16}---this
seems a natural fit.
\end{itemize}

\section{Really Want An Excuse To Haves}

As James Mickens puts it\cite{mickens2014saddest}:

\begin{quote}
``How can you make a reliable computer service?'' the presenter will ask in an innocent voice before continuing,
``It may be difficult if you can’t trust anything and the entire concept of happiness is a lie designed by unseen overlords of endless deceptive power.''
The presenter never explicitly says that last part,
but everybody understands what's happening.
\end{quote}

\input{./includes/bibsection.tex}

\end{document}
