\input{./includes/preamble.tex}

\title{PhD Report}
\author{David R. MacIver}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}

NB:\ This literature review is very much a work in progress and is not really ready for public consumption.
It is and will be highly fragmented,
with large missing gaps and much of the bits that are here only in a temporary state that will later be reworked into other things.
Proceed with caution.

Let me begin with some context.

The focus of my PhD is an open source project I wrote (and many others now also contribute to) called \emph{Hypothesis}.
Hypothesis was originally purely a Python library\footnote{\url{https://github.com/HypothesisWorks/hypothesis-python}},
but now also has a version for Ruby\footnote{\url{https://github.com/HypothesisWorks/hypothesis-ruby}}.
I will blur the distinction and simply refer to them all as ``Hypothesis''.

Hypothesis is a library in the spirit of QuickCheck\cite{DBLP:conf/icfp/ClaessenH00} for \emph{property-based testing}.
The distinguishing feature of such tools is that they make it easy for users to define tests that combine user-written specifications with some form of data generation.
Such tests assert properties that are intended to hold for an entire class of inputs,
although in practice only ever test on a finite number of inputs in any given run.

I do not propose to do an extensive review of the existing literature on QuickCheck,
for two main reasons:

The first is that it would frankly be redundant.
If you are interested in a detailed survey of QuickCheck and QuickCheck-related tools in the context of Haskell,
I refer you to Rudy Braquehais's recent PhD thesis\cite{matela2017tools},
where he ably summarizes it and a wide variety of related tools.

The second is that despite the shared lineage,
due to differing implementation strategies and contexts,
Hypothesis actually has comparably little in common with QuickCheck that they do not both share in common with a much broader area of testing research.

QuickCheck nevertheless provides the starting point for many of the \emph{problems} that I will motivate,
and where relevant I will of course first back to it and its descendants.

\chapter{QuickCheck in Context}

QuickCheck was introduced back in 2000,
in ``QuickCheck: a lightweight tool for random testing of Haskell programs''\cite{DBLP:conf/icfp/ClaessenH00}.
It has since been ported to many languages\footnote{\url{https://hypothesis.works/articles/quickcheck-in-every-language/}},
with the most notable one being the commercial Erlang implementation\cite{DBLP:conf/erlang/ArtsHJW06}.

A test in QuickCheck looks something like the following:

\begin{lstlisting}[language=Haskell]
import Test.QuickCheck (quickCheck, (==>), Property)
import Data.List (elem, delete)

prop_remove :: [Int] -> Int -> Property
prop_remove ls x = elem x ls ==> not (elem x (delete x ls))

main = quickCheck prop_remove
\end{lstlisting}

In this test we test the property that given a list of integers and an integer,
with the integer contained in the list,
after removing the integer from the list it is not longer present.

A similar property expressed in Hypothesis would be:

\begin{lstlisting}[language=Python]
from hypothesis import given, assume
import hypothesis.strategies as st


@given(st.lists(st.integers()), st.integers())
def test_deleting(ls, i):
    assume(i in ls)
    ls.remove(i)
    assert i not in ls
\end{lstlisting}

When run, these tests both do the same thing:
They randomly generate a list of integers and an integer,
if the integer is contained in the list they continue the test,
otherwise they mark it as invalid.
Once they have found a sufficient number of valid test cases that did not fail (100 by default) the test stops.
If prior to that point they have found a counter-example to the property,
they begin a process of test-case reduction to attempt to produce a smaller, simpler, example.

From a research point of view,
this is an extremely boring idea,
and the QuickCheck authors knew that when they proposed it.
The following quote is from~\cite{DBLP:conf/icfp/ClaessenH00}:

\begin{quote}
We have taken two relatively old ideas, namely specifications
as oracles and random testing, and found ways to make
them easily available to Haskell programmers.
\end{quote}

Fundamentally,
even at the time the paper was written nearly two decades ago,
QuickCheck was never an attempt to push back the boundaries of testing research,
but instead a paper about tool usability.
The fundamental question posed by the paper is not ``can we test software better than we are currently doing?'' but taking as granted that we \emph{can},
how do we get non-researchers to use these tools?
QuickCheck proved a fairly convincing answer to this question.

This answer highlights what I think is an interesting split between researchers and practitioners,
which is that they have very different views on the \emph{oracle problem}.

The oracle problem, put colloquially, is that you can't know whether a piece of software is correct unless you know what the software is supposed to do.
Thus every test consists (possibly implicitly) of two parts:

\begin{itemize}
\item A \emph{test case} consisting of a set of inputs which exercises the system under test (SUT) in some way.
\item A \emph{test oracle} which can determine whether the SUT behaved incorrectly on those inputs.
\end{itemize}

Typically test oracles are sound but not complete---that is,
if the test oracle says that the behaviour is \emph{incorrect} then the behaviour is actually incorrect,
but the behaviour may be incorrect in some way that the test oracle is unaware of.

The current research on identifying test oracles is well described in~\cite{DBLP:journals/tse/BarrHMSY15},
but to summarize along slightly different axes than they do,
the following are the main ways in which researchers currently look for test oracles:

\begin{itemize}
\item They use test oracles that only look for behaviour that is \emph{obviously wrong}.
e.g.\ segfaults, assertion failures, and deadlocks.
\item They derive a test oracle (either automatically or manually) from some specification of the behaviour of the software.
\item They use multiple implementations of the same specification to do differential testing---comparing
the implementations and seeing if they produce the same answer.
\item They attempt to derive the oracle from the current behaviour of the SUT,
effectively producing a regression test suite---this
can't determine whether the current behaviour is correct,
but allows you to determine when it changes.
\end{itemize}

(TODO:\ Give cites of prominent examples of the above)

Practitioners on the other hand adopt a quite different approach to finding test oracles:
They just write them,
typically in the host language,
as part of their normal testing process.

The big difference between a researcher and a practitioner in software testing is this:
A researcher considers the topic of test-case generation to be a fascinating object of study that they can devote a great deal of time to improving,
and finding a test oracle for a particular system under test involve lots of boring non-research work in order to determine the specific goals and operational constraints of the software are.
A workflow where you can just check out a repository from, say, GitHub, run your software against it, and see what happens is \emph{great},
because it scales very well to many different projects without requiring any project specific work.
while test-case generation is a distraction which has essentially no business value except to the degree that it takes them less time and gains them more insight into their software's behaviour\footnote{
You may note that I did not say ``finds more bugs''.
The role of testing as primarily a bug-finding activity is another major difference between researchers and practitioners---in
many ways testing in practice,
especially unit testing,
is perceived as a design tool as much as a bug-finding activity.
},
which limits the amount of time they are willing to invest into it.

This creates a fairly stark difference in the priorities of researchers and practitioners:
Each regards as important the part that the other would prefer to avoid.
In an ideal world,
this would be a great opportunity---the

Framed in this light,
QuickCheck is a library that takes advantage of that opportunity,
and is designed primarily around making test-case generation accessible to non-researchers by providing a composable library of combinators for defining test-case generators\footnote{
Although in the original QuickCheck these combinators were defined in terms of type classes,
and any given type was considered to have some canonical distribution of test cases,
but this has proven to be inessential.
The Erlang version of QuickCheck (written by at least one of the original authors of the Haskell version)\cite{DBLP:conf/erlang/ArtsHJW06},
adopts a purely combinator based approach,
as do many others.
Even Haskell users are coming around the the combinator based approach.
A recent contribution to the field is HedgeHog\footnote{\url{https://github.com/hedgehogqa}},
which is a Haskell library (with several ports to other language) taking the combinator based approach for defining generators}.

\chapter{Property-based testing without functional programming}

How well does this generalise?

Haskell users are a particularly easy ``market'' for correctness tools,
especially ones which are designed to evoke an impression of formal verification like QuickCheck\footnote{
I used to jokingly describe QuickCheck's motivation as ``we wan't to do formal verification but it's too hard, so lets do testing instead''.
Then various people who were around at the time told me that this was pretty much spot on,
so now I still describe it that way but it's no longer a joke.
}.

So a natural question is whether this property-based testing works well outside of functional programming languages\footnote{
In actual fact it is essentially impossible for this question to have a sensible yes or no answer.
Neither ``functional programming language'' nor ``property-based testing'' are terms with a particularly well defined meaning,
even before we get onto the entirely subjective question of what counts as ``working well''?
Therefore this question serves not really as one that we are trying to answer,
but one whose interpretation may suggest interesting directions for more specific investigations
}.

Certainly there seems to be a widespread belief that the answer is no.
For example, to quote from~\cite{matela2017tools}:

\begin{quote}
Usability in imperative languages Although still useful, property-based testing is a
bit less useful in the realm of imperative programming languages as property-based testing
benefits from testing functions and modules that have no side-effects.
\end{quote}

QuickCheck has at this point been ported\footnote{with varying degrees of faithfullness---Hypothesis
is probably an outlier in how completely unlike the original QuickCheck it is,
but most vary to some greater or lesser degree,
with only languages that are very close to Haskell able to do any sort of truly faithful port} to a large number of languages,
but until recently it seems to have been most widely used

However this doesn't appear to be true,
except in the sense that it is true that \emph{all} testing benefits from testing functions and modules that have no side effects.
Certainly there doesn't seem to be any difficulty in getting Python programmers to use Hypothesis.
At the time of this writing libraries.io, an indexing site for open source software,
lists 520 open source projects using Hypothesis\footnote{\url{https://libraries.io/pypi/hypothesis/usage}}.
Python is a dynamically typed, mostly object oriented, imperative language,
with relatively limited support for functional programming (e.g.\ the Python lambda implementation is restricted to simple expressions),
which is quite a contrast to Haskell.

\chapter{How Industry Looks at Testing Research}

The following is from Dan Luu's ``One week of bugs''\footnote{\url{https://danluu.com/everything-is-broken/}}:

\begin{quote}
Given that people aren't going to put any effort into testing, what's the best way to do it?

Property-based testing. Generative testing. Random testing. Concolic Testing (which was done long before the term was coined). Static analysis. Fuzzing. Statistical bug finding. There are lots of options. Some of them are actually the same thing because the terminology we use is inconsistent and buggy. I'm going to arbitrarily pick one to talk about, but they're all worth looking into.

People are often intimidated by these, though.
I've seen a lot of talks on these and they often make it sound like this stuff is really hard.
Csmith is 40k LOC.\ 
American Fuzzy Lop's compile-time instrumentation is smart enough to generate valid JPEGs.
Sixth Sense has the same kind of intelligence as American Fuzzy Lop in terms of exploration, and in addition, uses symbolic execution to exhaustively explore large portions of the state space; it will formally verify that your asserts hold if it's able to collapse the state space enough to exhaustively search it, otherwise it merely tries to get the best possible test coverage by covering different paths and states.\ In addition, it will use symbolic equivalence checking to check different versions of your code against each other.

That's all really impressive, but you don't need a formal methods PhD to do this stuff.
You can write a fuzzer that will shake out a lot of bugs in an hour.
Seriously.
I'm a bit embarrassed to link to this, but this fuzzer\footnote{\url{https://github.com/danluu/Fuzz.jl}} was written in about an hour and found 20--30 bugs,
including incorrect code generation,
and crashes on basic operations like multiplication and exponentiation.
My guess is that it would take another 2--3 hours to shake out another 20--30 bugs (with support for more types), and maybe another day of work to get another 20--30 (with very basic support for random expressions).
I don't mention this because it's good.
It's not.
It's totally heinous.
But that's the point.
You can throw together an absurd hack in an hour and it will turn out to be pretty useful.
\end{quote}

Dan Luu is not a researcher,
but interestingly the software he is talking about is the Julia programming language\cite{DBLP:journals/corr/abs-1209-5145},
which has a fairly extensive publication track record\footnote{\url{https://julialang.org/publications/}}. 

This anecdote suggests some key things:

\begin{itemize}
\item Very little in the way of advanced testing tools are currently being deployed in industry in any widespread manner.
\item Some of this is due to an intimidation factor---these tools seem complicated,
and as if they require as large a knowledge of the research field to use as they did to implement.
\item Most software as deployed is sufficiently bad that even the most basic of these tools (a hand-written random input generator with very little time investment put into it) can find a significant number of bugs.
\end{itemize}

I am not aware of any significant studies about the adoption of testing tools (and after several conversations with researchers in the field believe this is because no such studies currently exist),
but having spent some time trying to get people from industry 

\chapter{Random Testing is Good, Actually}

A common criticism of QuickCheck is its use of \emph{random testing}.

The two main criticisms implicit in this are:

\begin{itemize}
\item This makes tests non-repeatable (this typically comes from practitioners).
\item Random testing is a fairly ineffective technique and we know many better ones (this typically comes from researchers).
\end{itemize}

The first criticism is easily disposed of:
Every implementation of QuickCheck I am aware of (and certainly the original) explicitly makes use of a \emph{pseudo-random number generator} (PRNG) of some sort,
so the test is easily repeatable just by printing the seed that lead to it\footnote{Hypothesis actually has a more elaborate mechanism for repeating tests}.
This can be done either automatically or by printing the seed as part of the failing run.

The second is more interesting.
Partly the best response to it is that it is correct---purely
random testing \emph{is} very limited compared to more advanced options---but
it is not as strong as it may seem.
Random testing is good, actually.

The reason for this is three-fold:

\begin{itemize}
\item Random testing is \emph{cheap}.
It is extremely easy to work with, and tends to be much faster than smarter techniques.
\item Random testing scales well to arbitrarily complex software,
because it does not have to care about the complexity of the software at all!
\item Random testing is much more effective than naive arguments might suggest. A classic result\cite{DBLP:journals/tse/HamletT90} shows that partition testing for example is not substantially more effective than random testing,
and recent analysis\cite{DBLP:journals/pacmpl/MajumdarN18}, of the effectiveness of Jepsen (a random tester for distributed systems)\footnote{\url{https://jepsen.io/}} showed that in fact we should expect random testing to cover a large number of coverage targets in only logarithmically many test executions.
\item Most software is sufficiently buggy that \emph{any} improvement in testing will likely find significant bugs in it.
Given that most software has not yet been subjected to random testing,
random testing will tend to be sufficient for finding bugs in it even if there are many it doesn't find.
\end{itemize}

The following two perspectives from industry are informative.

The following comes from lcamtuf (Michal Zalewski)'s historical notes on the development of the American Fuzzy Lop (AFL) fuzzer\footnote{\url{http://lcamtuf.coredump.cx/afl/historical_notes.txt}}:

\begin{quote}
Speed.
It's genuinely hard to compete with brute force when your ``smart''
approach is resource-intensive. If your instrumentation makes it 10x more
likely to find a bug, but runs 100x slower, your users are getting a bad
deal.

To avoid starting with a handicap, afl-fuzz is meant to let you fuzz most of
the intended targets at roughly their native speed---so even if it doesn't
add value, you do not lose much.
\end{quote}

Thus we should expect that random testing does an excellent job of picking the low-hanging fruit in software that is not already tested this way,
and it is unlikely that we can get people to adopt better techniques unless we make it as easy or easier to adopt them as it is to adopt random testing.

\chapter{Composable Test Case Reduction}

As well as test-case \emph{generation},
and important part of QuickCheck's utility comes from test-case \emph{reduction}.

Test-case reduction is the process of taking a failing test case and turning it into one that is in some sense ``smaller''.
(TODO: Elaborate on this paragraph)

Although the initial version of QuickCheck did not come with test-case reduction,
it was proposed by a user even before the paper was published:

\begin{quote}
He [Andy Gill, a user of the library] made an improvement in the way QuickCheck reports counter examples.
Soemtimes, the counter examples found are very large,
and it is difficult to go back to the property and understand why it is a counter example.
However,
when the counter example is an element of a tree shaped datatype,
the problem can often be located in one of the sub-trees of the counter example found.
Gill extended the arbitrary class with an ew method

\begin{lstlisting}[language=Haskell]
smaller :: a -> [a]
\end{lstlisting}

which is intended to return a list of smaller, but similar values to its argument---for example, direct subtrees.
He adapted the \texttt{quickCheck} function so that when a counter example is found,
it tries to find a smaller one using this function.
In some cases much smaller counterexamples were found,
greatly reducing the time to understand the bug found.
\end{quote}

These days this is a somewhat unsurprising observation---delta
debugging was introduced as a method for test-case reduction in~\cite{DBLP:journals/tse/ZellerH02},
and the benefit of augmenting random unit tests with test-case reduction is well known and has been studied in e.g.~\cite{DBLP:conf/issre/LeiA05,DBLP:conf/pldi/RegehrCCEEY12}.
It was significantly more novel at the time,
as the QuickCheck paper was published only a few months after the delta-debugging one.

Unfortunately the approach as implemented in QuickCheck has some significant limitations,
at the root of which is that generation and reduction compose ``in different directions''.

Data generation naturally has a monadic structure,
but test-case reduction does not:
In order to turn a reducer for one type into a reducer for another,
you need functions mapping the type in both directions.
Given a reducer \(\texttt{smaller} :: a \to [a]\) and a function \(a \to b\) (or a function \(b \to a\)) there is no way to produce a non-trivial reducer \(b \to [b]\)---either
without both you lack one of the ingredients you need to synthesize a suitable \(b\).

Two approaches have been approached to fix this which would allow the natural monadic structure to retain test-case reduction:

\begin{enumerate}
\item In SmartCheck\cite{DBLP:conf/haskell/Pike14},
test-case reduction is fully generic and works for arbitrary algebraic data types.
\item In the rose-tree based approach proposed by Reid Draper in~\cite{FreeShrinking},

\chapter{Goals}

TODO:\ This chapter needs a better title.

The primary goal of my research can be framed as follows:
How can we create tools that are as easy for practitioners to use 

\chapter{Unincorporated Material}

This chapter gathers papers that I must/should/could include in my review,
along with some notes on them,
but does not attempt to do any significant amount of synthesis.

\section{Must Haves}

\subsection{QuickCheck Papers}

The foundational paper for my work is of course ``QuickCheck: a lightweight tool for random testing of Haskell programs''\cite{DBLP:conf/icfp/ClaessenH00}.

There are a number of other papers worth referencing:

\begin{itemize}
\item ``Testing telecoms software with quviq QuickCheck''\cite{DBLP:conf/erlang/ArtsHJW06}
\item ``SmartCheck: automatic and efficient counterexample reduction and generalization''\cite{DBLP:conf/haskell/Pike14} is one of the few papers about improving QuickCheck's test case reduction,
so definitely worth including.
\end{itemize}

\subsection{Test Case Reduction Papers}

\begin{itemize}
\item ``Simplifying failure-inducing input''\cite{DBLP:conf/issta/HildebrandtZ00} is the delta-debugging paper that you have to cite if you ever write anything about test-case reduction.
\item ``One test to rule them all''\cite{DBLP:conf/issta/GroceHK17} is particularly relevant to Hypothesis because of the relationship between normalization and its shortlex-minimization goal.
\item ``Minimization of Randomized Unit Test Cases''\cite{DBLP:conf/issre/LeiA05} is a good justification for the combination of random test case generation and test-case reduction.
\end{itemize}


\section{Should Haves}

\begin{itemize}
\item ``Targeted property-based testing''\cite{DBLP:conf/issta/LoscherS17} is about how you can do what is basically hill climbing with restart to better test properties.
This is relevant mostly as an example of why it's nice to be able to extend with new operations on generated data.
\item ``Why is random testing effective for partition tolerance bugs?''\cite{DBLP:journals/pacmpl/MajumdarN18} is an interesting argument that random testing can and should work.
\item ``Partition Testing Does Not Inspire Confidence``\cite{DBLP:journals/tse/HamletT90} is a good argument that random testing is pretty OK.\ 
\item ``Behind Human Error''\cite{BehindHumanError} contains a lot of lucid discussion about error that I think is valuable for this sort of work.
\item ``An Experimental Evaluation of the Assumption of Independence in Multiversion Programming''\cite{DBLP:journals/tse/KnightL86}---a
lot of property-based testing is basically multiversion programming to do differential testing.
\item ``Software Testing Research: Achievements, Challenges, Dreams''\cite{DBLP:conf/icse/Bertolino07} (for a good survey of where software testing research is and wants to go, and some useful basic questions).
\end{itemize}

\section{Could Haves}

\begin{itemize}
\item ``Can a Machine Design?''\cite{doi:10.1162/07479360152681083} contains a really nice account of different modes of human/computer codesign,
and how having the computer make things for a human to correct is much less stressful than having the computer correct the human.
\item ``When and what to automate in software testing? {A} multi-vocal literature review''\cite{DBLP:journals/infsof/GarousiM16}---this
seems a natural fit.
\end{itemize}

\section{Really Want An Excuse To Haves}

As James Mickens puts it\cite{mickens2014saddest}:

\begin{quote}
``How can you make a reliable computer service?'' the presenter will ask in an innocent voice before continuing,
``It may be difficult if you canâ€™t trust anything and the entire concept of happiness is a lie designed by unseen overlords of endless deceptive power.''
The presenter never explicitly says that last part,
but everybody understands what's happening.
\end{quote}


\input{./includes/bibsection.tex}

\end{document}
